{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "common-chair",
   "metadata": {},
   "source": [
    "## Regression _ Logistic Regression\n",
    "주로 이항분류에 사용됨. 분류 문제에서 사용할 수 있는 가장 간단한 모델<br>\n",
    "선형 결합을 통해 나온 결과를 토대로 예측하게 된다!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-electron",
   "metadata": {},
   "source": [
    "### with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unknown-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "blank-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "\n",
    "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA) # csv면 comma 기준 아닌가..?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cosmetic-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "suitable-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df= 0.0, analyzer = 'char', sublinear_tf = True, ngram_range = (1,3), max_features = 5000)\n",
    "\n",
    "X = vectorizer.fit_transform(reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "concrete-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "animated-enlargement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' a',\n",
       " ' aa',\n",
       " ' ab',\n",
       " ' ac',\n",
       " ' ad',\n",
       " ' ae',\n",
       " ' af',\n",
       " ' ag',\n",
       " ' ah',\n",
       " ' ai',\n",
       " ' ak',\n",
       " ' al',\n",
       " ' am',\n",
       " ' an',\n",
       " ' ap',\n",
       " ' ar',\n",
       " ' as',\n",
       " ' at',\n",
       " ' au',\n",
       " ' av',\n",
       " ' aw',\n",
       " ' ax',\n",
       " ' az',\n",
       " ' b',\n",
       " ' b ',\n",
       " ' ba',\n",
       " ' bb',\n",
       " ' be',\n",
       " ' bi',\n",
       " ' bl',\n",
       " ' bo',\n",
       " ' br',\n",
       " ' bu',\n",
       " ' by',\n",
       " ' c',\n",
       " ' c ',\n",
       " ' ca',\n",
       " ' ce',\n",
       " ' cg',\n",
       " ' ch',\n",
       " ' ci',\n",
       " ' cl',\n",
       " ' co',\n",
       " ' cr',\n",
       " ' cu',\n",
       " ' cy',\n",
       " ' d',\n",
       " ' da',\n",
       " ' de',\n",
       " ' di',\n",
       " ' do',\n",
       " ' dr',\n",
       " ' du',\n",
       " ' dv',\n",
       " ' dw',\n",
       " ' dy',\n",
       " ' e',\n",
       " ' e ',\n",
       " ' ea',\n",
       " ' eb',\n",
       " ' ec',\n",
       " ' ed',\n",
       " ' ee',\n",
       " ' ef',\n",
       " ' eg',\n",
       " ' ei',\n",
       " ' el',\n",
       " ' em',\n",
       " ' en',\n",
       " ' ep',\n",
       " ' eq',\n",
       " ' er',\n",
       " ' es',\n",
       " ' et',\n",
       " ' eu',\n",
       " ' ev',\n",
       " ' ex',\n",
       " ' ey',\n",
       " ' f',\n",
       " ' f ',\n",
       " ' fa',\n",
       " ' fb',\n",
       " ' fe',\n",
       " ' fi',\n",
       " ' fl',\n",
       " ' fo',\n",
       " ' fr',\n",
       " ' fu',\n",
       " ' fx',\n",
       " ' g',\n",
       " ' g ',\n",
       " ' ga',\n",
       " ' ge',\n",
       " ' gh',\n",
       " ' gi',\n",
       " ' gl',\n",
       " ' go',\n",
       " ' gr',\n",
       " ' gu',\n",
       " ' gw',\n",
       " ' gy',\n",
       " ' h',\n",
       " ' h ',\n",
       " ' ha',\n",
       " ' hb',\n",
       " ' he',\n",
       " ' hi',\n",
       " ' hm',\n",
       " ' ho',\n",
       " ' hu',\n",
       " ' hy',\n",
       " ' i',\n",
       " ' ia',\n",
       " ' ic',\n",
       " ' id',\n",
       " ' ig',\n",
       " ' ii',\n",
       " ' il',\n",
       " ' im',\n",
       " ' in',\n",
       " ' ir',\n",
       " ' is',\n",
       " ' it',\n",
       " ' iv',\n",
       " ' j',\n",
       " ' j ',\n",
       " ' ja',\n",
       " ' je',\n",
       " ' ji',\n",
       " ' jo',\n",
       " ' jr',\n",
       " ' ju',\n",
       " ' k',\n",
       " ' k ',\n",
       " ' ka',\n",
       " ' ke',\n",
       " ' kh',\n",
       " ' ki',\n",
       " ' kl',\n",
       " ' kn',\n",
       " ' ko',\n",
       " ' kr',\n",
       " ' ku',\n",
       " ' ky',\n",
       " ' l',\n",
       " ' l ',\n",
       " ' la',\n",
       " ' le',\n",
       " ' li',\n",
       " ' ll',\n",
       " ' lo',\n",
       " ' lu',\n",
       " ' ly',\n",
       " ' m',\n",
       " ' ma',\n",
       " ' mc',\n",
       " ' me',\n",
       " ' mg',\n",
       " ' mi',\n",
       " ' mm',\n",
       " ' mo',\n",
       " ' mr',\n",
       " ' ms',\n",
       " ' mu',\n",
       " ' my',\n",
       " ' n',\n",
       " ' n ',\n",
       " ' na',\n",
       " ' nd',\n",
       " ' ne',\n",
       " ' ni',\n",
       " ' no',\n",
       " ' nu',\n",
       " ' ny',\n",
       " ' o',\n",
       " ' oa',\n",
       " ' ob',\n",
       " ' oc',\n",
       " ' od',\n",
       " ' of',\n",
       " ' oh',\n",
       " ' oi',\n",
       " ' ok',\n",
       " ' ol',\n",
       " ' om',\n",
       " ' on',\n",
       " ' oo',\n",
       " ' op',\n",
       " ' or',\n",
       " ' os',\n",
       " ' ot',\n",
       " ' ou',\n",
       " ' ov',\n",
       " ' ow',\n",
       " ' oz',\n",
       " ' p',\n",
       " ' p ',\n",
       " ' pa',\n",
       " ' pe',\n",
       " ' pg',\n",
       " ' ph',\n",
       " ' pi',\n",
       " ' pl',\n",
       " ' po',\n",
       " ' pr',\n",
       " ' ps',\n",
       " ' pu',\n",
       " ' py',\n",
       " ' q',\n",
       " ' qu',\n",
       " ' r',\n",
       " ' r ',\n",
       " ' ra',\n",
       " ' rd',\n",
       " ' re',\n",
       " ' rh',\n",
       " ' ri',\n",
       " ' ro',\n",
       " ' ru',\n",
       " ' ry',\n",
       " ' s',\n",
       " ' sa',\n",
       " ' sc',\n",
       " ' se',\n",
       " ' sg',\n",
       " ' sh',\n",
       " ' si',\n",
       " ' sk',\n",
       " ' sl',\n",
       " ' sm',\n",
       " ' sn',\n",
       " ' so',\n",
       " ' sp',\n",
       " ' sq',\n",
       " ' st',\n",
       " ' su',\n",
       " ' sw',\n",
       " ' sy',\n",
       " ' t',\n",
       " ' ta',\n",
       " ' te',\n",
       " ' th',\n",
       " ' ti',\n",
       " ' to',\n",
       " ' tr',\n",
       " ' ts',\n",
       " ' tu',\n",
       " ' tv',\n",
       " ' tw',\n",
       " ' ty',\n",
       " ' u',\n",
       " ' u ',\n",
       " ' ug',\n",
       " ' uk',\n",
       " ' ul',\n",
       " ' um',\n",
       " ' un',\n",
       " ' up',\n",
       " ' ur',\n",
       " ' us',\n",
       " ' ut',\n",
       " ' v',\n",
       " ' v ',\n",
       " ' va',\n",
       " ' ve',\n",
       " ' vh',\n",
       " ' vi',\n",
       " ' vo',\n",
       " ' vs',\n",
       " ' vu',\n",
       " ' w',\n",
       " ' w ',\n",
       " ' wa',\n",
       " ' we',\n",
       " ' wh',\n",
       " ' wi',\n",
       " ' wo',\n",
       " ' wr',\n",
       " ' wu',\n",
       " ' ww',\n",
       " ' x',\n",
       " ' x ',\n",
       " ' y',\n",
       " ' ya',\n",
       " ' ye',\n",
       " ' yo',\n",
       " ' yu',\n",
       " ' z',\n",
       " ' za',\n",
       " ' ze',\n",
       " ' zi',\n",
       " ' zo',\n",
       " ' zu',\n",
       " 'a',\n",
       " 'a ',\n",
       " 'a a',\n",
       " 'a b',\n",
       " 'a c',\n",
       " 'a d',\n",
       " 'a e',\n",
       " 'a f',\n",
       " 'a g',\n",
       " 'a h',\n",
       " 'a i',\n",
       " 'a j',\n",
       " 'a k',\n",
       " 'a l',\n",
       " 'a m',\n",
       " 'a n',\n",
       " 'a o',\n",
       " 'a p',\n",
       " 'a q',\n",
       " 'a r',\n",
       " 'a s',\n",
       " 'a t',\n",
       " 'a u',\n",
       " 'a v',\n",
       " 'a w',\n",
       " 'a y',\n",
       " 'aa',\n",
       " 'aa ',\n",
       " 'aaa',\n",
       " 'aar',\n",
       " 'ab',\n",
       " 'ab ',\n",
       " 'aba',\n",
       " 'abb',\n",
       " 'abc',\n",
       " 'abe',\n",
       " 'abh',\n",
       " 'abi',\n",
       " 'abl',\n",
       " 'abo',\n",
       " 'abr',\n",
       " 'abs',\n",
       " 'abu',\n",
       " 'aby',\n",
       " 'ac',\n",
       " 'ac ',\n",
       " 'aca',\n",
       " 'acc',\n",
       " 'ace',\n",
       " 'ach',\n",
       " 'aci',\n",
       " 'ack',\n",
       " 'acl',\n",
       " 'aco',\n",
       " 'acq',\n",
       " 'acr',\n",
       " 'act',\n",
       " 'acu',\n",
       " 'acy',\n",
       " 'ad',\n",
       " 'ad ',\n",
       " 'ada',\n",
       " 'adc',\n",
       " 'add',\n",
       " 'ade',\n",
       " 'adf',\n",
       " 'adg',\n",
       " 'adh',\n",
       " 'adi',\n",
       " 'adj',\n",
       " 'adl',\n",
       " 'adm',\n",
       " 'adn',\n",
       " 'ado',\n",
       " 'adr',\n",
       " 'ads',\n",
       " 'adu',\n",
       " 'adv',\n",
       " 'adw',\n",
       " 'ady',\n",
       " 'ae',\n",
       " 'ae ',\n",
       " 'ael',\n",
       " 'aes',\n",
       " 'af',\n",
       " 'af ',\n",
       " 'afe',\n",
       " 'aff',\n",
       " 'afi',\n",
       " 'afo',\n",
       " 'afr',\n",
       " 'aft',\n",
       " 'ag',\n",
       " 'ag ',\n",
       " 'aga',\n",
       " 'age',\n",
       " 'agg',\n",
       " 'agh',\n",
       " 'agi',\n",
       " 'agl',\n",
       " 'agn',\n",
       " 'ago',\n",
       " 'agr',\n",
       " 'ags',\n",
       " 'agu',\n",
       " 'ah',\n",
       " 'ah ',\n",
       " 'aha',\n",
       " 'ahe',\n",
       " 'ahi',\n",
       " 'aho',\n",
       " 'ai',\n",
       " 'ai ',\n",
       " 'aic',\n",
       " 'aid',\n",
       " 'aig',\n",
       " 'ail',\n",
       " 'aim',\n",
       " 'ain',\n",
       " 'air',\n",
       " 'ais',\n",
       " 'ait',\n",
       " 'aiv',\n",
       " 'aj',\n",
       " 'aj ',\n",
       " 'aja',\n",
       " 'ajo',\n",
       " 'ak',\n",
       " 'ak ',\n",
       " 'aka',\n",
       " 'ake',\n",
       " 'aki',\n",
       " 'akn',\n",
       " 'ako',\n",
       " 'aks',\n",
       " 'aku',\n",
       " 'aky',\n",
       " 'al',\n",
       " 'al ',\n",
       " 'ala',\n",
       " 'alb',\n",
       " 'alc',\n",
       " 'ald',\n",
       " 'ale',\n",
       " 'alf',\n",
       " 'alg',\n",
       " 'ali',\n",
       " 'alk',\n",
       " 'all',\n",
       " 'alm',\n",
       " 'alo',\n",
       " 'alp',\n",
       " 'alr',\n",
       " 'als',\n",
       " 'alt',\n",
       " 'alu',\n",
       " 'alv',\n",
       " 'alw',\n",
       " 'aly',\n",
       " 'am',\n",
       " 'am ',\n",
       " 'ama',\n",
       " 'amb',\n",
       " 'ame',\n",
       " 'ami',\n",
       " 'aml',\n",
       " 'amm',\n",
       " 'amn',\n",
       " 'amo',\n",
       " 'amp',\n",
       " 'amr',\n",
       " 'ams',\n",
       " 'amu',\n",
       " 'amy',\n",
       " 'an',\n",
       " 'an ',\n",
       " 'ana',\n",
       " 'anc',\n",
       " 'and',\n",
       " 'ane',\n",
       " 'ang',\n",
       " 'anh',\n",
       " 'ani',\n",
       " 'ank',\n",
       " 'anl',\n",
       " 'ann',\n",
       " 'ano',\n",
       " 'ans',\n",
       " 'ant',\n",
       " 'anu',\n",
       " 'anw',\n",
       " 'anx',\n",
       " 'any',\n",
       " 'anz',\n",
       " 'ao',\n",
       " 'ao ',\n",
       " 'aor',\n",
       " 'ap',\n",
       " 'ap ',\n",
       " 'apa',\n",
       " 'ape',\n",
       " 'aph',\n",
       " 'api',\n",
       " 'apl',\n",
       " 'apo',\n",
       " 'app',\n",
       " 'apr',\n",
       " 'aps',\n",
       " 'apt',\n",
       " 'aq',\n",
       " 'aq ',\n",
       " 'aqu',\n",
       " 'ar',\n",
       " 'ar ',\n",
       " 'ara',\n",
       " 'arb',\n",
       " 'arc',\n",
       " 'ard',\n",
       " 'are',\n",
       " 'arf',\n",
       " 'arg',\n",
       " 'ari',\n",
       " 'ark',\n",
       " 'arl',\n",
       " 'arm',\n",
       " 'arn',\n",
       " 'aro',\n",
       " 'arp',\n",
       " 'arq',\n",
       " 'arr',\n",
       " 'ars',\n",
       " 'art',\n",
       " 'arv',\n",
       " 'ary',\n",
       " 'arz',\n",
       " 'as',\n",
       " 'as ',\n",
       " 'asa',\n",
       " 'asc',\n",
       " 'ase',\n",
       " 'ash',\n",
       " 'asi',\n",
       " 'ask',\n",
       " 'asl',\n",
       " 'asm',\n",
       " 'aso',\n",
       " 'asp',\n",
       " 'ass',\n",
       " 'ast',\n",
       " 'asu',\n",
       " 'asy',\n",
       " 'at',\n",
       " 'at ',\n",
       " 'ata',\n",
       " 'atc',\n",
       " 'ate',\n",
       " 'ath',\n",
       " 'ati',\n",
       " 'atl',\n",
       " 'atm',\n",
       " 'atn',\n",
       " 'ato',\n",
       " 'atr',\n",
       " 'ats',\n",
       " 'att',\n",
       " 'atu',\n",
       " 'aty',\n",
       " 'au',\n",
       " 'au ',\n",
       " 'auc',\n",
       " 'aud',\n",
       " 'aug',\n",
       " 'aul',\n",
       " 'aum',\n",
       " 'aun',\n",
       " 'aur',\n",
       " 'aus',\n",
       " 'aut',\n",
       " 'av',\n",
       " 'ava',\n",
       " 'ave',\n",
       " 'avi',\n",
       " 'avo',\n",
       " 'avy',\n",
       " 'aw',\n",
       " 'aw ',\n",
       " 'awa',\n",
       " 'awe',\n",
       " 'awf',\n",
       " 'awi',\n",
       " 'awk',\n",
       " 'awl',\n",
       " 'awn',\n",
       " 'awr',\n",
       " 'aws',\n",
       " 'awy',\n",
       " 'ax',\n",
       " 'ax ',\n",
       " 'axe',\n",
       " 'axi',\n",
       " 'ay',\n",
       " 'ay ',\n",
       " 'aya',\n",
       " 'ayb',\n",
       " 'aye',\n",
       " 'ayi',\n",
       " 'ayl',\n",
       " 'aym',\n",
       " 'ayn',\n",
       " 'ayo',\n",
       " 'ays',\n",
       " 'ayw',\n",
       " 'az',\n",
       " 'az ',\n",
       " 'aza',\n",
       " 'aze',\n",
       " 'azi',\n",
       " 'azo',\n",
       " 'azy',\n",
       " 'azz',\n",
       " 'b',\n",
       " 'b ',\n",
       " 'b a',\n",
       " 'b b',\n",
       " 'b c',\n",
       " 'b d',\n",
       " 'b e',\n",
       " 'b f',\n",
       " 'b g',\n",
       " 'b h',\n",
       " 'b i',\n",
       " 'b j',\n",
       " 'b l',\n",
       " 'b m',\n",
       " 'b n',\n",
       " 'b o',\n",
       " 'b p',\n",
       " 'b r',\n",
       " 'b s',\n",
       " 'b t',\n",
       " 'b u',\n",
       " 'b w',\n",
       " 'ba',\n",
       " 'ba ',\n",
       " 'bab',\n",
       " 'bac',\n",
       " 'bad',\n",
       " 'baf',\n",
       " 'bag',\n",
       " 'bai',\n",
       " 'bak',\n",
       " 'bal',\n",
       " 'ban',\n",
       " 'bar',\n",
       " 'bas',\n",
       " 'bat',\n",
       " 'bau',\n",
       " 'bay',\n",
       " 'bb',\n",
       " 'bba',\n",
       " 'bbc',\n",
       " 'bbe',\n",
       " 'bbi',\n",
       " 'bbl',\n",
       " 'bbo',\n",
       " 'bby',\n",
       " 'bc',\n",
       " 'bc ',\n",
       " 'bd',\n",
       " 'be',\n",
       " 'be ',\n",
       " 'bea',\n",
       " 'bec',\n",
       " 'bed',\n",
       " 'bee',\n",
       " 'bef',\n",
       " 'beg',\n",
       " 'beh',\n",
       " 'bei',\n",
       " 'bel',\n",
       " 'ben',\n",
       " 'ber',\n",
       " 'bes',\n",
       " 'bet',\n",
       " 'bew',\n",
       " 'bey',\n",
       " 'bh',\n",
       " 'bha',\n",
       " 'bi',\n",
       " 'bi ',\n",
       " 'bia',\n",
       " 'bib',\n",
       " 'bic',\n",
       " 'bid',\n",
       " 'bie',\n",
       " 'big',\n",
       " 'bik',\n",
       " 'bil',\n",
       " 'bin',\n",
       " 'bio',\n",
       " 'bir',\n",
       " 'bis',\n",
       " 'bit',\n",
       " 'biz',\n",
       " 'bj',\n",
       " 'bje',\n",
       " 'bl',\n",
       " 'bla',\n",
       " 'ble',\n",
       " 'bli',\n",
       " 'blo',\n",
       " 'blu',\n",
       " 'bly',\n",
       " 'bm',\n",
       " 'bn',\n",
       " 'bno',\n",
       " 'bo',\n",
       " 'bo ',\n",
       " 'boa',\n",
       " 'bob',\n",
       " 'bod',\n",
       " 'bog',\n",
       " 'boi',\n",
       " 'bol',\n",
       " 'bom',\n",
       " 'bon',\n",
       " 'boo',\n",
       " 'bor',\n",
       " 'bos',\n",
       " 'bot',\n",
       " 'bou',\n",
       " 'bow',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'bp',\n",
       " 'bpl',\n",
       " 'br',\n",
       " 'bra',\n",
       " 'bre',\n",
       " 'bri',\n",
       " 'bro',\n",
       " 'bru',\n",
       " 'bs',\n",
       " 'bs ',\n",
       " 'bsc',\n",
       " 'bse',\n",
       " 'bsi',\n",
       " 'bso',\n",
       " 'bst',\n",
       " 'bsu',\n",
       " 'bt',\n",
       " 'bt ',\n",
       " 'bta',\n",
       " 'bte',\n",
       " 'bti',\n",
       " 'btl',\n",
       " 'bu',\n",
       " 'bu ',\n",
       " 'bub',\n",
       " 'buc',\n",
       " 'bud',\n",
       " 'buf',\n",
       " 'bug',\n",
       " 'bui',\n",
       " 'bul',\n",
       " 'bum',\n",
       " 'bun',\n",
       " 'bur',\n",
       " 'bus',\n",
       " 'but',\n",
       " 'buy',\n",
       " 'bv',\n",
       " 'bvi',\n",
       " 'by',\n",
       " 'by ',\n",
       " 'bye',\n",
       " 'byr',\n",
       " 'bys',\n",
       " 'c',\n",
       " 'c ',\n",
       " 'c a',\n",
       " 'c b',\n",
       " 'c c',\n",
       " 'c d',\n",
       " 'c e',\n",
       " 'c f',\n",
       " 'c g',\n",
       " 'c h',\n",
       " 'c i',\n",
       " 'c j',\n",
       " 'c k',\n",
       " 'c l',\n",
       " 'c m',\n",
       " 'c n',\n",
       " 'c o',\n",
       " 'c p',\n",
       " 'c r',\n",
       " 'c s',\n",
       " 'c t',\n",
       " 'c u',\n",
       " 'c v',\n",
       " 'c w',\n",
       " 'c y',\n",
       " 'ca',\n",
       " 'ca ',\n",
       " 'cab',\n",
       " 'cad',\n",
       " 'cag',\n",
       " 'cai',\n",
       " 'cak',\n",
       " 'cal',\n",
       " 'cam',\n",
       " 'can',\n",
       " 'cap',\n",
       " 'car',\n",
       " 'cas',\n",
       " 'cat',\n",
       " 'cau',\n",
       " 'cav',\n",
       " 'cb',\n",
       " 'cc',\n",
       " 'cca',\n",
       " 'cce',\n",
       " 'cci',\n",
       " 'ccl',\n",
       " 'cco',\n",
       " 'ccu',\n",
       " 'cd',\n",
       " 'cdo',\n",
       " 'ce',\n",
       " 'ce ',\n",
       " 'cea',\n",
       " 'cec',\n",
       " 'ced',\n",
       " 'cee',\n",
       " 'cef',\n",
       " 'cei',\n",
       " 'cel',\n",
       " 'cem',\n",
       " 'cen',\n",
       " 'cep',\n",
       " 'cer',\n",
       " 'ces',\n",
       " 'cet',\n",
       " 'cey',\n",
       " 'cg',\n",
       " 'cgi',\n",
       " 'ch',\n",
       " 'ch ',\n",
       " 'cha',\n",
       " 'chc',\n",
       " 'che',\n",
       " 'chi',\n",
       " 'chl',\n",
       " 'chm',\n",
       " 'chn',\n",
       " 'cho',\n",
       " 'chr',\n",
       " 'cht',\n",
       " 'chu',\n",
       " 'chw',\n",
       " 'chy',\n",
       " 'ci',\n",
       " 'ci ',\n",
       " 'cia',\n",
       " 'cid',\n",
       " 'cie',\n",
       " 'cif',\n",
       " 'cig',\n",
       " 'cil',\n",
       " 'cin',\n",
       " 'cio',\n",
       " 'cip',\n",
       " 'cir',\n",
       " 'cis',\n",
       " 'cit',\n",
       " 'civ',\n",
       " 'ciz',\n",
       " 'ck',\n",
       " 'ck ',\n",
       " 'cka',\n",
       " 'ckb',\n",
       " 'ckd',\n",
       " 'cke',\n",
       " 'ckg',\n",
       " 'ckh',\n",
       " 'cki',\n",
       " 'ckl',\n",
       " 'ckm',\n",
       " 'ckn',\n",
       " 'cko',\n",
       " 'cks',\n",
       " 'ckt',\n",
       " 'ckw',\n",
       " 'cky',\n",
       " 'cl',\n",
       " 'cla',\n",
       " 'cle',\n",
       " 'cli',\n",
       " 'clo',\n",
       " 'clu',\n",
       " 'cm',\n",
       " 'cn',\n",
       " 'co',\n",
       " 'co ',\n",
       " 'coa',\n",
       " 'cob',\n",
       " 'coc',\n",
       " 'cod',\n",
       " 'coe',\n",
       " 'cof',\n",
       " 'cog',\n",
       " 'coh',\n",
       " 'coi',\n",
       " 'col',\n",
       " 'com',\n",
       " 'con',\n",
       " 'coo',\n",
       " 'cop',\n",
       " 'cor',\n",
       " 'cos',\n",
       " 'cot',\n",
       " 'cou',\n",
       " 'cov',\n",
       " 'cow',\n",
       " 'cox',\n",
       " 'coy',\n",
       " 'cq',\n",
       " 'cqu',\n",
       " 'cr',\n",
       " 'cra',\n",
       " 'cre',\n",
       " 'cri',\n",
       " 'cro',\n",
       " 'cru',\n",
       " 'cry',\n",
       " 'cs',\n",
       " 'cs ',\n",
       " 'ct',\n",
       " 'ct ',\n",
       " 'cta',\n",
       " 'cte',\n",
       " 'cti',\n",
       " 'ctl',\n",
       " 'cto',\n",
       " 'ctr',\n",
       " 'cts',\n",
       " 'ctu',\n",
       " 'cu',\n",
       " 'cub',\n",
       " 'cue',\n",
       " 'cul',\n",
       " 'cum',\n",
       " 'cun',\n",
       " 'cup',\n",
       " 'cur',\n",
       " 'cus',\n",
       " 'cut',\n",
       " 'cy',\n",
       " 'cy ',\n",
       " 'cyb',\n",
       " 'cyc',\n",
       " 'cyn',\n",
       " 'cz',\n",
       " 'd',\n",
       " 'd ',\n",
       " 'd a',\n",
       " 'd b',\n",
       " 'd c',\n",
       " 'd d',\n",
       " 'd e',\n",
       " 'd f',\n",
       " 'd g',\n",
       " 'd h',\n",
       " 'd i',\n",
       " 'd j',\n",
       " 'd k',\n",
       " 'd l',\n",
       " 'd m',\n",
       " 'd n',\n",
       " 'd o',\n",
       " 'd p',\n",
       " 'd q',\n",
       " 'd r',\n",
       " 'd s',\n",
       " 'd t',\n",
       " 'd u',\n",
       " 'd v',\n",
       " 'd w',\n",
       " 'd y',\n",
       " 'd z',\n",
       " 'da',\n",
       " 'da ',\n",
       " 'dab',\n",
       " 'dac',\n",
       " 'dad',\n",
       " 'dag',\n",
       " 'dah',\n",
       " 'dai',\n",
       " 'dal',\n",
       " 'dam',\n",
       " 'dan',\n",
       " 'dap',\n",
       " 'dar',\n",
       " 'das',\n",
       " 'dat',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = vectorizer.get_feature_names()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "infrared-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "southwest-exception",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증데이터 분리\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "paperback-spokesman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 선언 및 학습\n",
    "\n",
    "lgs = LogisticRegression(class_weight = 'balanced')\n",
    "lgs.fit(X_train, y_train) # 이걸로 학습 시켜라! lgs 모델을"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "instructional-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = lgs.predict(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hourly-millennium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터로 성능 평가\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(lgs.score(X_eval, y_eval))) # lgs.score 함수를 쓰면 X_eval 자리에 있는 걸 lgs 모델로 예측하고 y_eval과 비교한 accuracy를 알려주나보군!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fatty-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 데이터에서 만족할만큼 나오면 평가 데이터 적용\n",
    "\n",
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "insured-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataVecs = vectorizer.transform(test_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "coated-electric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "test_predicted = lgs.predict(testDataVecs)\n",
    "print(test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "known-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "    \n",
    "answer_dataset = pd.DataFrame({'id': test_data['id'], 'sentiment': test_predicted})\n",
    "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_tfidf_answer.csv', index= False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-detection",
   "metadata": {},
   "source": [
    "### with word2vec\n",
    "\n",
    "입력: 단어로 표현된 리스트 (전처리한 넘파이 배열 사용하지 X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "alive-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "pleasant-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in reviews: # reviews 안에 들어있는 건 다 train_data\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "simplified-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "min_word_count = 40 # 단어에 대한 최소 빈도 수\n",
    "num_workers = 4 # 프로세스 개수\n",
    "context = 10 # 컨텍스트 윈도우 크기\n",
    "downsampling = 1e-3 # 다운 샘플링 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fallen-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                   level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "placed-morrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 01:46:00,657 : INFO : collecting all words and their counts\n",
      "2021-03-24 01:46:00,658 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-03-24 01:46:00,844 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2021-03-24 01:46:01,014 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2021-03-24 01:46:01,101 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2021-03-24 01:46:01,102 : INFO : Loading a fresh vocabulary\n",
      "2021-03-24 01:46:01,132 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2021-03-24 01:46:01,133 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
      "2021-03-24 01:46:01,153 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2021-03-24 01:46:01,155 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2021-03-24 01:46:01,156 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
      "2021-03-24 01:46:01,175 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2021-03-24 01:46:01,176 : INFO : resetting layer weights\n",
      "2021-03-24 01:46:01,274 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2021-03-24 01:46:02,282 : INFO : EPOCH 1 - PROGRESS: at 45.44% examples, 1137119 words/s, in_qsize 8, out_qsize 0\n",
      "2021-03-24 01:46:03,296 : INFO : EPOCH 1 - PROGRESS: at 93.84% examples, 1161180 words/s, in_qsize 8, out_qsize 0\n",
      "2021-03-24 01:46:03,395 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-24 01:46:03,396 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-24 01:46:03,403 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-24 01:46:03,409 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-24 01:46:03,410 : INFO : EPOCH - 1 : training on 2988089 raw words (2494639 effective words) took 2.1s, 1170201 effective words/s\n",
      "2021-03-24 01:46:04,424 : INFO : EPOCH 2 - PROGRESS: at 42.16% examples, 1047493 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-24 01:46:05,435 : INFO : EPOCH 2 - PROGRESS: at 77.68% examples, 962340 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-24 01:46:05,924 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-24 01:46:05,929 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-24 01:46:05,931 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-24 01:46:05,940 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-24 01:46:05,941 : INFO : EPOCH - 2 : training on 2988089 raw words (2494137 effective words) took 2.5s, 987004 effective words/s\n",
      "2021-03-24 01:46:06,955 : INFO : EPOCH 3 - PROGRESS: at 39.47% examples, 982030 words/s, in_qsize 8, out_qsize 0\n",
      "2021-03-24 01:46:07,963 : INFO : EPOCH 3 - PROGRESS: at 84.64% examples, 1049729 words/s, in_qsize 8, out_qsize 0\n",
      "2021-03-24 01:46:08,275 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-24 01:46:08,277 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-24 01:46:08,280 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-24 01:46:08,284 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-24 01:46:08,285 : INFO : EPOCH - 3 : training on 2988089 raw words (2494144 effective words) took 2.3s, 1065565 effective words/s\n",
      "2021-03-24 01:46:09,294 : INFO : EPOCH 4 - PROGRESS: at 43.72% examples, 1094057 words/s, in_qsize 8, out_qsize 0\n",
      "2021-03-24 01:46:10,294 : INFO : EPOCH 4 - PROGRESS: at 84.99% examples, 1060696 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-24 01:46:10,644 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-24 01:46:10,651 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-24 01:46:10,661 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-24 01:46:10,663 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-24 01:46:10,664 : INFO : EPOCH - 4 : training on 2988089 raw words (2494105 effective words) took 2.4s, 1049961 effective words/s\n",
      "2021-03-24 01:46:11,676 : INFO : EPOCH 5 - PROGRESS: at 42.76% examples, 1067217 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-24 01:46:12,681 : INFO : EPOCH 5 - PROGRESS: at 92.05% examples, 1143220 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-24 01:46:12,839 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-24 01:46:12,845 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-24 01:46:12,851 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-24 01:46:12,854 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-24 01:46:12,855 : INFO : EPOCH - 5 : training on 2988089 raw words (2494669 effective words) took 2.2s, 1140479 effective words/s\n",
      "2021-03-24 01:46:12,856 : INFO : training on a 14940445 raw words (12471694 effective words) took 11.6s, 1076845 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# word2vec 학습: Word2Vec 객체를 생성해서 실행.\n",
    "# 이렇게 학습하고 생성된 객체는 model 변수에 할당 (학습을 위한 객체의 인자는 입력할 데이터와 하이퍼파라미터를 순서대로 입력해야 원하는 하이퍼파라미터를 사용해 학습할 수 있음!!!)\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers = num_workers,\n",
    "                         size = num_features, min_count = min_word_count,\n",
    "                         window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "# 모델 저장 시 하이퍼파라미터 설정한 내용을 모델 이름에 담으면 참고하기 좋음\n",
    "# 모델을 저장하면 Word2Vec.load()를 통해 모델을 다시 사용할 수 있음\n",
    "\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name) # model이 앞에 만든 word2vec 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "contained-villa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.Word2Vec"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "forbidden-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 리뷰당 하나의 벡터로 만들기. 문장에 있는 모든 단어의 벡터값에 대해 평균을 내는 방법\n",
    "def get_features(words, model, num_features):\n",
    "    # 출력 벡터 초기화\n",
    "    feature_vector = np.zeros((num_features), dtype = np.float32)\n",
    "    \n",
    "    num_words = 0\n",
    "    # 어휘 사전 준비\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words += 1\n",
    "            # 사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
    "            feature_vector = np.add(feature_vector, model[w]) # model에 인덱스가 있어..?? 그런가 봄.. integer은 안된다고 하고 str만 가능!\n",
    "            \n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "single-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 리뷰에 위의 함수 적용하는 함수 만들기\n",
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "    \n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(s, model, num_features))\n",
    "        \n",
    "    \n",
    "    reviewFeatureVecs = np.stack(dataset) # np.stack의 기능은?\n",
    "    \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "charged-toner",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_data_vecs = get_dataset(sentences, model, num_features) # 이제야 학습에 사용할 리뷰별 벡터로 준비됨 (sentences는 train_Data에서 온 아이)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "smoking-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 데이터 셋 분리\n",
    "\n",
    "X = test_data_vecs\n",
    "y = np.array(sentiments)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = TEST_SPLIT, random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "wrapped-functionality",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgs = LogisticRegression(class_weight = 'balanced')\n",
    "lgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "skilled-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2f}'.format(lgs.score(X_test, y_test))) # 검증 데이터로 성능(accuracy) 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "grateful-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출할 데이터 만들기\n",
    "\n",
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
    "\n",
    "test_review = list(test_data['review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "documentary-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = []\n",
    "for review in test_review:\n",
    "    test_sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "metric-industry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_data_vecs = get_dataset(test_sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "basic-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './data_out/'\n",
    "test_predicted = lgs.predict(test_data_vecs)\n",
    "\n",
    "ids= list(test_data['id'])\n",
    "answer_dataset = pd.DataFrame({'id': ids, 'sentiment': test_predicted})\n",
    "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_w2v_answer.csv', index = False, quoting =3) # quoting 3 뭐가 달라지는 걸까..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-passion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
