{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "printable-singing",
   "metadata": {},
   "source": [
    "all exercises based on [Introduction to Deep Learning for NLP](https://wikidocs.net/111472)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "noted-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "operational-professional",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=8.0>]\n"
     ]
    }
   ],
   "source": [
    "# tape_gradient() : 자동 미분 기능 수행\n",
    "w = tf.Variable(2.)\n",
    "\n",
    "def f(w):\n",
    "    y = w**2\n",
    "    z = 2*y + 5\n",
    "    return z\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w)\n",
    "    \n",
    "gradients = tape.gradient(z, [w]) # z를 w에 대해 미분\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-revelation",
   "metadata": {},
   "source": [
    "### 자동 미분을 이용한 선형 회귀 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "civic-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습될 가중치 변수 선언\n",
    "# 학습될 값이므로 일단 임의의 값으로 설정\n",
    "W = tf.Variable(4.0)\n",
    "b = tf.Variable(1.0) # 이렇게 tf.Variable로 하는 이유? 학습될 매개변수. 그냥 숫자 할당하면 뒤에 리스트를 변수로 넣었을 때 안돌아감 * 표시를 리스트 곱하는 걸로 인식해서?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "taken-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function # 이건 뭐지 (https://www.tensorflow.org/guide/function?hl=ko) -> 코랩에서 실습\n",
    "def hypothesis(x):\n",
    "    return W*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "opening-smooth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21. 15. 23. 25.]\n"
     ]
    }
   ],
   "source": [
    "x_test = [5,3.5, 5.5,6]\n",
    "print(hypothesis(x_test).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "excess-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# practice\n",
    "# W_ = 4\n",
    "# b_ = 1\n",
    "# def f(x):\n",
    "#     return W_*x + b_\n",
    "# print(f(x_test).numpy())\n",
    "# ----> error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abstract-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def mse_loss(y_pred, y):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "consolidated-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "X = [1, 2, 3, 4, 5, 6, 7, 8, 9] # 공부 시간\n",
    "y = [11, 22, 33, 44, 55, 66, 77, 87, 95] # 각 공부 시간에 맵핑되는 성적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "existing-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.SGD(0.01) # optimizer은 경사하강법, 학습률 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "documented-there",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.636882"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.numpy() # 그냥 W 만 쓰면 tf.Variable 객체라 숫자 값이 안 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "thick-liquid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :   0 | y_pred :  tf.Tensor(\n",
      "[11.956942 22.593822 33.230705 43.867588 54.50447  65.14135  75.77824\n",
      " 86.415115 97.051994], shape=(9,), dtype=float32)\n",
      "epoch :   0 | cost : tf.Tensor(0.92971396, shape=(), dtype=float32)\n",
      "epoch :   0 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.019103527>, <tf.Tensor: shape=(), dtype=float32, numpy=0.120049804>]\n",
      "epoch :   0 | W의 값 : 10.6371 | b의 값 : 1.3189 | cost : 0.929714\n",
      "epoch :  10 | y_pred :  tf.Tensor(\n",
      "[11.947029 22.585785 33.22454  43.863297 54.502052 65.14081  75.779564\n",
      " 86.41832  97.057076], shape=(9,), dtype=float32)\n",
      "epoch :  10 | cost : tf.Tensor(0.92829484, shape=(), dtype=float32)\n",
      "epoch :  10 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.018288612>, <tf.Tensor: shape=(), dtype=float32, numpy=0.11521575>]\n",
      "epoch :  10 | W의 값 : 10.6389 | b의 값 : 1.3071 | cost : 0.928295\n",
      "epoch :  20 | y_pred :  tf.Tensor(\n",
      "[11.937516 22.57807  33.218624 43.859177 54.49973  65.14029  75.78084\n",
      " 86.421394 97.06195 ], shape=(9,), dtype=float32)\n",
      "epoch :  20 | cost : tf.Tensor(0.9269871, shape=(), dtype=float32)\n",
      "epoch :  20 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.017512321>, <tf.Tensor: shape=(), dtype=float32, numpy=0.11057514>]\n",
      "epoch :  20 | W의 값 : 10.6407 | b의 값 : 1.2959 | cost : 0.926987\n",
      "epoch :  30 | y_pred :  tf.Tensor(\n",
      "[11.928387 22.570665 33.212944 43.855225 54.497505 65.13978  75.78206\n",
      " 86.42434  97.06662 ], shape=(9,), dtype=float32)\n",
      "epoch :  30 | cost : tf.Tensor(0.92578137, shape=(), dtype=float32)\n",
      "epoch :  30 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.016811848>, <tf.Tensor: shape=(), dtype=float32, numpy=0.10611597>]\n",
      "epoch :  30 | W의 값 : 10.6424 | b의 값 : 1.2850 | cost : 0.925781\n",
      "epoch :  40 | y_pred :  tf.Tensor(\n",
      "[11.919625 22.56356  33.207493 43.85143  54.495365 65.1393   75.783226\n",
      " 86.42716  97.0711  ], shape=(9,), dtype=float32)\n",
      "epoch :  40 | cost : tf.Tensor(0.92467034, shape=(), dtype=float32)\n",
      "epoch :  40 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.016152382>, <tf.Tensor: shape=(), dtype=float32, numpy=0.101834804>]\n",
      "epoch :  40 | W의 값 : 10.6441 | b의 값 : 1.2747 | cost : 0.924670\n",
      "epoch :  50 | y_pred :  tf.Tensor(\n",
      "[11.911217 22.55674  33.20226  43.847782 54.493305 65.138824 75.78435\n",
      " 86.42987  97.07539 ], shape=(9,), dtype=float32)\n",
      "epoch :  50 | cost : tf.Tensor(0.9236482, shape=(), dtype=float32)\n",
      "epoch :  50 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.015552521>, <tf.Tensor: shape=(), dtype=float32, numpy=0.09771982>]\n",
      "epoch :  50 | W의 값 : 10.6457 | b의 값 : 1.2647 | cost : 0.923648\n",
      "epoch :  60 | y_pred :  tf.Tensor(\n",
      "[11.903149 22.550196 33.197243 43.844288 54.491337 65.13838  75.78543\n",
      " 86.43248  97.07953 ], shape=(9,), dtype=float32)\n",
      "epoch :  60 | cost : tf.Tensor(0.92270726, shape=(), dtype=float32)\n",
      "epoch :  60 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.014886856>, <tf.Tensor: shape=(), dtype=float32, numpy=0.09378517>]\n",
      "epoch :  60 | W의 값 : 10.6472 | b의 값 : 1.2552 | cost : 0.922707\n",
      "epoch :  70 | y_pred :  tf.Tensor(\n",
      "[11.895405 22.543915 33.192425 43.840935 54.489445 65.137955 75.78646\n",
      " 86.434975 97.08349 ], shape=(9,), dtype=float32)\n",
      "epoch :  70 | cost : tf.Tensor(0.92184067, shape=(), dtype=float32)\n",
      "epoch :  70 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.014307022>, <tf.Tensor: shape=(), dtype=float32, numpy=0.09000057>]\n",
      "epoch :  70 | W의 값 : 10.6487 | b의 값 : 1.2460 | cost : 0.921841\n",
      "epoch :  80 | y_pred :  tf.Tensor(\n",
      "[11.887973 22.537886 33.1878   43.837715 54.48763  65.13754  75.78745\n",
      " 86.43737  97.08729 ], shape=(9,), dtype=float32)\n",
      "epoch :  80 | cost : tf.Tensor(0.9210415, shape=(), dtype=float32)\n",
      "epoch :  80 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.013748169>, <tf.Tensor: shape=(), dtype=float32, numpy=0.08636835>]\n",
      "epoch :  80 | W의 값 : 10.6501 | b의 값 : 1.2372 | cost : 0.921041\n",
      "epoch :  90 | y_pred :  tf.Tensor(\n",
      "[11.880841 22.532103 33.183365 43.834625 54.485886 65.13715  75.788414\n",
      " 86.439674 97.090935], shape=(9,), dtype=float32)\n",
      "epoch :  90 | cost : tf.Tensor(0.92030185, shape=(), dtype=float32)\n",
      "epoch :  90 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.013178349>, <tf.Tensor: shape=(), dtype=float32, numpy=0.08288807>]\n",
      "epoch :  90 | W의 값 : 10.6514 | b의 값 : 1.2288 | cost : 0.920302\n",
      "epoch : 100 | y_pred :  tf.Tensor(\n",
      "[11.873998 22.526552 33.179108 43.83166  54.484215 65.13677  75.78932\n",
      " 86.44188  97.09444 ], shape=(9,), dtype=float32)\n",
      "epoch : 100 | cost : tf.Tensor(0.9196276, shape=(), dtype=float32)\n",
      "epoch : 100 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.012667656>, <tf.Tensor: shape=(), dtype=float32, numpy=0.079543024>]\n",
      "epoch : 100 | W의 값 : 10.6527 | b의 값 : 1.2206 | cost : 0.919628\n",
      "epoch : 110 | y_pred :  tf.Tensor(\n",
      "[11.867431 22.521227 33.175022 43.82882  54.482616 65.136406 75.79021\n",
      " 86.444    97.09779 ], shape=(9,), dtype=float32)\n",
      "epoch : 110 | cost : tf.Tensor(0.9190002, shape=(), dtype=float32)\n",
      "epoch : 110 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.01214695>, <tf.Tensor: shape=(), dtype=float32, numpy=0.07633823>]\n",
      "epoch : 110 | W의 값 : 10.6539 | b의 값 : 1.2129 | cost : 0.919000\n",
      "epoch : 120 | y_pred :  tf.Tensor(\n",
      "[11.861126 22.516113 33.171097 43.826084 54.48107  65.136055 75.791046\n",
      " 86.44603  97.10101 ], shape=(9,), dtype=float32)\n",
      "epoch : 120 | cost : tf.Tensor(0.91842544, shape=(), dtype=float32)\n",
      "epoch : 120 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.011701107>, <tf.Tensor: shape=(), dtype=float32, numpy=0.07325238>]\n",
      "epoch : 120 | W의 값 : 10.6551 | b의 값 : 1.2054 | cost : 0.918425\n",
      "epoch : 130 | y_pred :  tf.Tensor(\n",
      "[11.855078 22.511208 33.167336 43.823467 54.479595 65.13573  75.791855\n",
      " 86.44798  97.10411 ], shape=(9,), dtype=float32)\n",
      "epoch : 130 | cost : tf.Tensor(0.91789603, shape=(), dtype=float32)\n",
      "epoch : 130 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.011211395>, <tf.Tensor: shape=(), dtype=float32, numpy=0.07030192>]\n",
      "epoch : 130 | W의 값 : 10.6562 | b의 값 : 1.1982 | cost : 0.917896\n",
      "epoch : 140 | y_pred :  tf.Tensor(\n",
      "[11.849272 22.506498 33.163727 43.820953 54.47818  65.13541  75.79263\n",
      " 86.44986  97.107086], shape=(9,), dtype=float32)\n",
      "epoch : 140 | cost : tf.Tensor(0.91740966, shape=(), dtype=float32)\n",
      "epoch : 140 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.010736465>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0674701>]\n",
      "epoch : 140 | W의 값 : 10.6573 | b의 값 : 1.1914 | cost : 0.917410\n",
      "epoch : 150 | y_pred :  tf.Tensor(\n",
      "[11.843701 22.50198  33.16026  43.81854  54.476818 65.1351   75.79338\n",
      " 86.45166  97.10994 ], shape=(9,), dtype=float32)\n",
      "epoch : 150 | cost : tf.Tensor(0.91696084, shape=(), dtype=float32)\n",
      "epoch : 150 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.010290146>, <tf.Tensor: shape=(), dtype=float32, numpy=0.06475091>]\n",
      "epoch : 150 | W의 값 : 10.6584 | b의 값 : 1.1848 | cost : 0.916961\n",
      "epoch : 160 | y_pred :  tf.Tensor(\n",
      "[11.838356 22.497646 33.156937 43.816227 54.475517 65.1348   75.7941\n",
      " 86.453384 97.11267 ], shape=(9,), dtype=float32)\n",
      "epoch : 160 | cost : tf.Tensor(0.91654533, shape=(), dtype=float32)\n",
      "epoch : 160 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.0098724365>, <tf.Tensor: shape=(), dtype=float32, numpy=0.062142253>]\n",
      "epoch : 160 | W의 값 : 10.6594 | b의 값 : 1.1784 | cost : 0.916545\n",
      "epoch : 170 | y_pred :  tf.Tensor(\n",
      "[11.833224 22.493484 33.153748 43.814007 54.474266 65.13453  75.794785\n",
      " 86.45505  97.11531 ], shape=(9,), dtype=float32)\n",
      "epoch : 170 | cost : tf.Tensor(0.9161682, shape=(), dtype=float32)\n",
      "epoch : 170 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.009414196>, <tf.Tensor: shape=(), dtype=float32, numpy=0.05964473>]\n",
      "epoch : 170 | W의 값 : 10.6604 | b의 값 : 1.1724 | cost : 0.916168\n",
      "epoch : 180 | y_pred :  tf.Tensor(\n",
      "[11.8283  22.48949 33.15068 43.81187 54.47306 65.13425 75.79543 86.45663\n",
      " 97.11782], shape=(9,), dtype=float32)\n",
      "epoch : 180 | cost : tf.Tensor(0.9158194, shape=(), dtype=float32)\n",
      "epoch : 180 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.009105682>, <tf.Tensor: shape=(), dtype=float32, numpy=0.057228923>]\n",
      "epoch : 180 | W의 값 : 10.6613 | b의 값 : 1.1665 | cost : 0.915819\n",
      "epoch : 190 | y_pred :  tf.Tensor(\n",
      "[11.823575 22.485657 33.14774  43.809822 54.471905 65.13399  75.796074\n",
      " 86.45815  97.12023 ], shape=(9,), dtype=float32)\n",
      "epoch : 190 | cost : tf.Tensor(0.91549045, shape=(), dtype=float32)\n",
      "epoch : 190 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.008740425>, <tf.Tensor: shape=(), dtype=float32, numpy=0.054920822>]\n",
      "epoch : 190 | W의 값 : 10.6622 | b의 값 : 1.1609 | cost : 0.915490\n",
      "epoch : 200 | y_pred :  tf.Tensor(\n",
      "[11.81904  22.48198  33.14492  43.80786  54.470802 65.133736 75.79668\n",
      " 86.45962  97.12256 ], shape=(9,), dtype=float32)\n",
      "epoch : 200 | cost : tf.Tensor(0.91519654, shape=(), dtype=float32)\n",
      "epoch : 200 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.008369446>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0527094>]\n",
      "epoch : 200 | W의 값 : 10.6630 | b의 값 : 1.1556 | cost : 0.915197\n",
      "epoch : 210 | y_pred :  tf.Tensor(\n",
      "[11.814688 22.47845  33.142212 43.805973 54.469734 65.1335   75.79726\n",
      " 86.46102  97.12479 ], shape=(9,), dtype=float32)\n",
      "epoch : 210 | cost : tf.Tensor(0.9149238, shape=(), dtype=float32)\n",
      "epoch : 210 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.008038521>, <tf.Tensor: shape=(), dtype=float32, numpy=0.05058247>]\n",
      "epoch : 210 | W의 값 : 10.6638 | b의 값 : 1.1504 | cost : 0.914924\n",
      "epoch : 220 | y_pred :  tf.Tensor(\n",
      "[11.810511 22.475061 33.13961  43.80416  54.46871  65.13326  75.79781\n",
      " 86.462364 97.126915], shape=(9,), dtype=float32)\n",
      "epoch : 220 | cost : tf.Tensor(0.91466975, shape=(), dtype=float32)\n",
      "epoch : 220 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.0077590942>, <tf.Tensor: shape=(), dtype=float32, numpy=0.048535675>]\n",
      "epoch : 220 | W의 값 : 10.6646 | b의 값 : 1.1455 | cost : 0.914670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 230 | y_pred :  tf.Tensor(\n",
      "[11.806503 22.471813 33.13712  43.80243  54.46774  65.13305  75.79836\n",
      " 86.46367  97.128975], shape=(9,), dtype=float32)\n",
      "epoch : 230 | cost : tf.Tensor(0.91443485, shape=(), dtype=float32)\n",
      "epoch : 230 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.00736475>, <tf.Tensor: shape=(), dtype=float32, numpy=0.046591014>]\n",
      "epoch : 230 | W의 값 : 10.6654 | b의 값 : 1.1407 | cost : 0.914435\n",
      "epoch : 240 | y_pred :  tf.Tensor(\n",
      "[11.802657 22.468693 33.134727 43.800762 54.466797 65.132835 75.79887\n",
      " 86.464905 97.13094 ], shape=(9,), dtype=float32)\n",
      "epoch : 240 | cost : tf.Tensor(0.9142253, shape=(), dtype=float32)\n",
      "epoch : 240 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.0070972443>, <tf.Tensor: shape=(), dtype=float32, numpy=0.044708073>]\n",
      "epoch : 240 | W의 값 : 10.6661 | b의 값 : 1.1362 | cost : 0.914225\n",
      "epoch : 250 | y_pred :  tf.Tensor(\n",
      "[11.798965 22.465698 33.13243  43.799164 54.465897 65.13263  75.79936\n",
      " 86.466095 97.13283 ], shape=(9,), dtype=float32)\n",
      "epoch : 250 | cost : tf.Tensor(0.91402584, shape=(), dtype=float32)\n",
      "epoch : 250 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.006819248>, <tf.Tensor: shape=(), dtype=float32, numpy=0.042904347>]\n",
      "epoch : 250 | W의 값 : 10.6668 | b의 값 : 1.1318 | cost : 0.914026\n",
      "epoch : 260 | y_pred :  tf.Tensor(\n",
      "[11.795423 22.462826 33.130226 43.79763  54.465034 65.13243  75.799835\n",
      " 86.46724  97.13464 ], shape=(9,), dtype=float32)\n",
      "epoch : 260 | cost : tf.Tensor(0.9138459, shape=(), dtype=float32)\n",
      "epoch : 260 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.0065364838>, <tf.Tensor: shape=(), dtype=float32, numpy=0.041175216>]\n",
      "epoch : 260 | W의 값 : 10.6675 | b의 값 : 1.1276 | cost : 0.913846\n",
      "epoch : 270 | y_pred :  tf.Tensor(\n",
      "[11.792023 22.460068 33.128113 43.796158 54.464203 65.13225  75.80029\n",
      " 86.46834  97.13638 ], shape=(9,), dtype=float32)\n",
      "epoch : 270 | cost : tf.Tensor(0.913677, shape=(), dtype=float32)\n",
      "epoch : 270 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.0062594414>, <tf.Tensor: shape=(), dtype=float32, numpy=0.039516896>]\n",
      "epoch : 270 | W의 값 : 10.6681 | b의 값 : 1.1236 | cost : 0.913677\n",
      "epoch : 280 | y_pred :  tf.Tensor(\n",
      "[11.78876  22.45742  33.12608  43.794743 54.463406 65.132065 75.80073\n",
      " 86.46939  97.138054], shape=(9,), dtype=float32)\n",
      "epoch : 280 | cost : tf.Tensor(0.9135246, shape=(), dtype=float32)\n",
      "epoch : 280 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.006014824>, <tf.Tensor: shape=(), dtype=float32, numpy=0.03792125>]\n",
      "epoch : 280 | W의 값 : 10.6687 | b의 값 : 1.1197 | cost : 0.913525\n",
      "epoch : 290 | y_pred :  tf.Tensor(\n",
      "[11.785628 22.454882 33.124134 43.793385 54.462635 65.1319   75.80115\n",
      " 86.4704   97.13965 ], shape=(9,), dtype=float32)\n",
      "epoch : 290 | cost : tf.Tensor(0.91337925, shape=(), dtype=float32)\n",
      "epoch : 290 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.0057907104>, <tf.Tensor: shape=(), dtype=float32, numpy=0.036389858>]\n",
      "epoch : 290 | W의 값 : 10.6693 | b의 값 : 1.1160 | cost : 0.913379\n",
      "epoch : 300 | y_pred :  tf.Tensor(\n",
      "[11.782624 22.452446 33.122265 43.792088 54.46191  65.13172  75.801544\n",
      " 86.47137  97.14119 ], shape=(9,), dtype=float32)\n",
      "epoch : 300 | cost : tf.Tensor(0.9132527, shape=(), dtype=float32)\n",
      "epoch : 300 | gradients:  [<tf.Tensor: shape=(), dtype=float32, numpy=-0.0055570602>, <tf.Tensor: shape=(), dtype=float32, numpy=0.034923345>]\n",
      "epoch : 300 | W의 값 : 10.6699 | b의 값 : 1.1125 | cost : 0.913253\n"
     ]
    }
   ],
   "source": [
    "# 300번에 걸쳐 경사하강법 수행할 것\n",
    "for i in range(301):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 현재 파라미터에 기반한 입력 x에 대한 예측값을 y_pred\n",
    "        y_pred = hypothesis(X)\n",
    "        if i%10 == 0:\n",
    "            print(\"epoch : {:3} | y_pred : \".format(i), y_pred)\n",
    "        # 평균 제곱 오차 계산\n",
    "        cost = mse_loss(y_pred, y)\n",
    "        if i%10 == 0:\n",
    "            print(\"epoch : {:3} | cost :\".format(i), cost)\n",
    "        \n",
    "    # 손실 함수에 대한 파라미터의 미분값 계산 (W, b에 대해 모두 gradients를 구하나? 왜 gradients가 2개의 element로 구성된 것인지)\n",
    "    gradients = tape.gradient(cost, [W, b]) # 왜 with 사용하는 범위 안에서 안 쓰고 밖에서 tape를 쓰지??\n",
    "    if i%10 == 0:\n",
    "        print(\"epoch : {:3} | gradients: \".format(i), gradients)\n",
    "    # 파라미터 업데이트\n",
    "    optimizer.apply_gradients(zip(gradients, [W,b])) # zip 기능?\n",
    "    \n",
    "    if i%10 ==0:\n",
    "        print(\"epoch : {:3} | W의 값 : {:5.4f} | b의 값 : {:5.4f} | cost : {:5.6f}\".format(i, W.numpy(), b.numpy(), cost))\n",
    "        \n",
    "# W, b 값이 업데이트됨에 따라 cost가 지속적으로 줄어듦."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-jersey",
   "metadata": {},
   "source": [
    "zip(*iterable)은 동일한 개수로 이루어진 자료형을 묶어 주는 역할을 하는 함수이다.\n",
    "\n",
    "※ 여기서 사용한 *iterable은 반복 가능(iterable)한 자료형 여러 개를 입력할 수 있다는 의미이다.\n",
    "\n",
    "잘 이해되지 않는다면 다음 예제를 살펴보자.\n",
    "```\n",
    ">>> list(zip([1, 2, 3], [4, 5, 6]))\n",
    "[(1, 4), (2, 5), (3, 6)]\n",
    ">>> list(zip([1, 2, 3], [4, 5, 6], [7, 8, 9]))\n",
    "[(1, 4, 7), (2, 5, 8), (3, 6, 9)]\n",
    ">>> list(zip(\"abc\", \"def\"))\n",
    "[('a', 'd'), ('b', 'e'), ('c', 'f')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rocky-miniature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38.45702  54.461834 59.796772 65.131714]\n"
     ]
    }
   ],
   "source": [
    "x_test = [3.5, 5, 5.5, 6]\n",
    "print(hypothesis(x_test).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-watts",
   "metadata": {},
   "source": [
    "### 케라스로 구현하기 (선형회귀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "flexible-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "turned-vegetation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "9/9 [==============================] - 0s 885us/step - loss: 375.4327 - mse: 375.4327\n",
      "Epoch 2/300\n",
      "9/9 [==============================] - 0s 776us/step - loss: 1.8896 - mse: 1.8896\n",
      "Epoch 3/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8891 - mse: 1.8891\n",
      "Epoch 4/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8886 - mse: 1.8886\n",
      "Epoch 5/300\n",
      "9/9 [==============================] - 0s 781us/step - loss: 1.8882 - mse: 1.8882\n",
      "Epoch 6/300\n",
      "9/9 [==============================] - 0s 780us/step - loss: 1.8877 - mse: 1.8877\n",
      "Epoch 7/300\n",
      "9/9 [==============================] - 0s 894us/step - loss: 1.8873 - mse: 1.8873\n",
      "Epoch 8/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8869 - mse: 1.8869\n",
      "Epoch 9/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8865 - mse: 1.8865\n",
      "Epoch 10/300\n",
      "9/9 [==============================] - 0s 777us/step - loss: 1.8861 - mse: 1.8861\n",
      "Epoch 11/300\n",
      "9/9 [==============================] - 0s 888us/step - loss: 1.8857 - mse: 1.8857\n",
      "Epoch 12/300\n",
      "9/9 [==============================] - 0s 777us/step - loss: 1.8853 - mse: 1.8853\n",
      "Epoch 13/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8850 - mse: 1.8850\n",
      "Epoch 14/300\n",
      "9/9 [==============================] - 0s 777us/step - loss: 1.8846 - mse: 1.8846\n",
      "Epoch 15/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8843 - mse: 1.8843\n",
      "Epoch 16/300\n",
      "9/9 [==============================] - 0s 780us/step - loss: 1.8840 - mse: 1.8840\n",
      "Epoch 17/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8837 - mse: 1.8837\n",
      "Epoch 18/300\n",
      "9/9 [==============================] - 0s 781us/step - loss: 1.8834 - mse: 1.8834\n",
      "Epoch 19/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8831 - mse: 1.8831\n",
      "Epoch 20/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8828 - mse: 1.8828\n",
      "Epoch 21/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8825 - mse: 1.8825\n",
      "Epoch 22/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8822 - mse: 1.8822\n",
      "Epoch 23/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8820 - mse: 1.8820\n",
      "Epoch 24/300\n",
      "9/9 [==============================] - 0s 888us/step - loss: 1.8818 - mse: 1.8818\n",
      "Epoch 25/300\n",
      "9/9 [==============================] - 0s 893us/step - loss: 1.8815 - mse: 1.8815\n",
      "Epoch 26/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8813 - mse: 1.8813\n",
      "Epoch 27/300\n",
      "9/9 [==============================] - 0s 888us/step - loss: 1.8811 - mse: 1.8811\n",
      "Epoch 28/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8809 - mse: 1.8809\n",
      "Epoch 29/300\n",
      "9/9 [==============================] - 0s 777us/step - loss: 1.8807 - mse: 1.8807\n",
      "Epoch 30/300\n",
      "9/9 [==============================] - 0s 668us/step - loss: 1.8805 - mse: 1.8805\n",
      "Epoch 31/300\n",
      "9/9 [==============================] - 0s 773us/step - loss: 1.8803 - mse: 1.8803\n",
      "Epoch 32/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8801 - mse: 1.8801\n",
      "Epoch 33/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8799 - mse: 1.8799\n",
      "Epoch 34/300\n",
      "9/9 [==============================] - 0s 1000us/step - loss: 1.8797 - mse: 1.8797\n",
      "Epoch 35/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8796 - mse: 1.8796\n",
      "Epoch 36/300\n",
      "9/9 [==============================] - 0s 1000us/step - loss: 1.8794 - mse: 1.8794\n",
      "Epoch 37/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8792 - mse: 1.8792\n",
      "Epoch 38/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8791 - mse: 1.8791\n",
      "Epoch 39/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.8789 - mse: 1.8789\n",
      "Epoch 40/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8788 - mse: 1.8788\n",
      "Epoch 41/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.8786 - mse: 1.8786\n",
      "Epoch 42/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.8785 - mse: 1.8785\n",
      "Epoch 43/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.8784 - mse: 1.8784\n",
      "Epoch 44/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8783 - mse: 1.8783\n",
      "Epoch 45/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.8782 - mse: 1.8782\n",
      "Epoch 46/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.8780 - mse: 1.8780\n",
      "Epoch 47/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.8779 - mse: 1.8779\n",
      "Epoch 48/300\n",
      "9/9 [==============================] - 0s 890us/step - loss: 1.8778 - mse: 1.8778\n",
      "Epoch 49/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8777 - mse: 1.8777\n",
      "Epoch 50/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8776 - mse: 1.8776\n",
      "Epoch 51/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8775 - mse: 1.8775\n",
      "Epoch 52/300\n",
      "9/9 [==============================] - 0s 779us/step - loss: 1.8774 - mse: 1.8774\n",
      "Epoch 53/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8773 - mse: 1.8773\n",
      "Epoch 54/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8772 - mse: 1.8772\n",
      "Epoch 55/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8772 - mse: 1.8772\n",
      "Epoch 56/300\n",
      "9/9 [==============================] - 0s 887us/step - loss: 1.8771 - mse: 1.8771\n",
      "Epoch 57/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8770 - mse: 1.8770\n",
      "Epoch 58/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8769 - mse: 1.8769\n",
      "Epoch 59/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8768 - mse: 1.8768\n",
      "Epoch 60/300\n",
      "9/9 [==============================] - 0s 670us/step - loss: 1.8768 - mse: 1.8768\n",
      "Epoch 61/300\n",
      "9/9 [==============================] - 0s 777us/step - loss: 1.8767 - mse: 1.8767\n",
      "Epoch 62/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8766 - mse: 1.8766\n",
      "Epoch 63/300\n",
      "9/9 [==============================] - 0s 781us/step - loss: 1.8766 - mse: 1.8766\n",
      "Epoch 64/300\n",
      "9/9 [==============================] - 0s 665us/step - loss: 1.8765 - mse: 1.8765\n",
      "Epoch 65/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8764 - mse: 1.8764\n",
      "Epoch 66/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8764 - mse: 1.8764\n",
      "Epoch 67/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8763 - mse: 1.8763\n",
      "Epoch 68/300\n",
      "9/9 [==============================] - 0s 665us/step - loss: 1.8763 - mse: 1.8763\n",
      "Epoch 69/300\n",
      "9/9 [==============================] - 0s 670us/step - loss: 1.8762 - mse: 1.8762\n",
      "Epoch 70/300\n",
      "9/9 [==============================] - 0s 781us/step - loss: 1.8762 - mse: 1.8762\n",
      "Epoch 71/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8761 - mse: 1.8761\n",
      "Epoch 72/300\n",
      "9/9 [==============================] - 0s 774us/step - loss: 1.8761 - mse: 1.8761\n",
      "Epoch 73/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8760 - mse: 1.8760\n",
      "Epoch 74/300\n",
      "9/9 [==============================] - 0s 670us/step - loss: 1.8760 - mse: 1.8760\n",
      "Epoch 75/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8759 - mse: 1.8759\n",
      "Epoch 76/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8759 - mse: 1.8759\n",
      "Epoch 77/300\n",
      "9/9 [==============================] - 0s 669us/step - loss: 1.8758 - mse: 1.8758\n",
      "Epoch 78/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8758 - mse: 1.8758\n",
      "Epoch 79/300\n",
      "9/9 [==============================] - 0s 670us/step - loss: 1.8758 - mse: 1.8758\n",
      "Epoch 80/300\n",
      "9/9 [==============================] - 0s 777us/step - loss: 1.8757 - mse: 1.8757\n",
      "Epoch 81/300\n",
      "9/9 [==============================] - 0s 1000us/step - loss: 1.8757 - mse: 1.8757\n",
      "Epoch 82/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8757 - mse: 1.8757\n",
      "Epoch 83/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8756 - mse: 1.8756\n",
      "Epoch 84/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8756 - mse: 1.8756\n",
      "Epoch 85/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8756 - mse: 1.8756\n",
      "Epoch 86/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8755 - mse: 1.8755\n",
      "Epoch 87/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 777us/step - loss: 1.8755 - mse: 1.8755\n",
      "Epoch 88/300\n",
      "9/9 [==============================] - 0s 669us/step - loss: 1.8755 - mse: 1.8755\n",
      "Epoch 89/300\n",
      "9/9 [==============================] - 0s 782us/step - loss: 1.8754 - mse: 1.8754\n",
      "Epoch 90/300\n",
      "9/9 [==============================] - 0s 890us/step - loss: 1.8754 - mse: 1.8754\n",
      "Epoch 91/300\n",
      "9/9 [==============================] - 0s 776us/step - loss: 1.8754 - mse: 1.8754\n",
      "Epoch 92/300\n",
      "9/9 [==============================] - 0s 773us/step - loss: 1.8754 - mse: 1.8754\n",
      "Epoch 93/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8753 - mse: 1.8753\n",
      "Epoch 94/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8753 - mse: 1.8753\n",
      "Epoch 95/300\n",
      "9/9 [==============================] - 0s 891us/step - loss: 1.8753 - mse: 1.8753\n",
      "Epoch 96/300\n",
      "9/9 [==============================] - 0s 774us/step - loss: 1.8753 - mse: 1.8753\n",
      "Epoch 97/300\n",
      "9/9 [==============================] - 0s 669us/step - loss: 1.8753 - mse: 1.8753\n",
      "Epoch 98/300\n",
      "9/9 [==============================] - 0s 560us/step - loss: 1.8752 - mse: 1.8752\n",
      "Epoch 99/300\n",
      "9/9 [==============================] - 0s 781us/step - loss: 1.8752 - mse: 1.8752\n",
      "Epoch 100/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8752 - mse: 1.8752\n",
      "Epoch 101/300\n",
      "9/9 [==============================] - 0s 671us/step - loss: 1.8752 - mse: 1.8752\n",
      "Epoch 102/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8752 - mse: 1.8752\n",
      "Epoch 103/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8751 - mse: 1.8751\n",
      "Epoch 104/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8751 - mse: 1.8751\n",
      "Epoch 105/300\n",
      "9/9 [==============================] - 0s 671us/step - loss: 1.8751 - mse: 1.8751\n",
      "Epoch 106/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8751 - mse: 1.8751\n",
      "Epoch 107/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8751 - mse: 1.8751\n",
      "Epoch 108/300\n",
      "9/9 [==============================] - 0s 775us/step - loss: 1.8751 - mse: 1.8751\n",
      "Epoch 109/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8750 - mse: 1.8750\n",
      "Epoch 110/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8750 - mse: 1.8750\n",
      "Epoch 111/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8750 - mse: 1.8750\n",
      "Epoch 112/300\n",
      "9/9 [==============================] - 0s 773us/step - loss: 1.8750 - mse: 1.8750\n",
      "Epoch 113/300\n",
      "9/9 [==============================] - 0s 665us/step - loss: 1.8750 - mse: 1.8750\n",
      "Epoch 114/300\n",
      "9/9 [==============================] - 0s 671us/step - loss: 1.8750 - mse: 1.8750\n",
      "Epoch 115/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8750 - mse: 1.8750\n",
      "Epoch 116/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8750 - mse: 1.8750\n",
      "Epoch 117/300\n",
      "9/9 [==============================] - 0s 777us/step - loss: 1.8750 - mse: 1.8750\n",
      "Epoch 118/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8749 - mse: 1.8749\n",
      "Epoch 119/300\n",
      "9/9 [==============================] - 0s 777us/step - loss: 1.8749 - mse: 1.8749\n",
      "Epoch 120/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8749 - mse: 1.8749\n",
      "Epoch 121/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8749 - mse: 1.8749\n",
      "Epoch 122/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8749 - mse: 1.8749\n",
      "Epoch 123/300\n",
      "9/9 [==============================] - 0s 1000us/step - loss: 1.8749 - mse: 1.8749\n",
      "Epoch 124/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8749 - mse: 1.8749\n",
      "Epoch 125/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8749 - mse: 1.8749\n",
      "Epoch 126/300\n",
      "9/9 [==============================] - 0s 777us/step - loss: 1.8749 - mse: 1.8749\n",
      "Epoch 127/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8749 - mse: 1.8749\n",
      "Epoch 128/300\n",
      "9/9 [==============================] - 0s 888us/step - loss: 1.8749 - mse: 1.8749\n",
      "Epoch 129/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.8749 - mse: 1.8749\n",
      "Epoch 130/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 131/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 132/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 133/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 134/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 135/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 136/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 137/300\n",
      "9/9 [==============================] - 0s 779us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 138/300\n",
      "9/9 [==============================] - 0s 892us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 139/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 140/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 141/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 142/300\n",
      "9/9 [==============================] - 0s 665us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 143/300\n",
      "9/9 [==============================] - 0s 777us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 144/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 145/300\n",
      "9/9 [==============================] - 0s 891us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 146/300\n",
      "9/9 [==============================] - 0s 664us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 147/300\n",
      "9/9 [==============================] - 0s 558us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 148/300\n",
      "9/9 [==============================] - 0s 670us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 149/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8748 - mse: 1.8748\n",
      "Epoch 150/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 151/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 152/300\n",
      "9/9 [==============================] - 0s 776us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 153/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 154/300\n",
      "9/9 [==============================] - 0s 664us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 155/300\n",
      "9/9 [==============================] - 0s 554us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 156/300\n",
      "9/9 [==============================] - 0s 668us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 157/300\n",
      "9/9 [==============================] - 0s 777us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 158/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 159/300\n",
      "9/9 [==============================] - 0s 668us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 160/300\n",
      "9/9 [==============================] - 0s 779us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 161/300\n",
      "9/9 [==============================] - 0s 890us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 162/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 163/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 164/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 165/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 166/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 167/300\n",
      "9/9 [==============================] - 0s 673us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 168/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 169/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 170/300\n",
      "9/9 [==============================] - 0s 663us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 171/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 172/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 668us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 173/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 174/300\n",
      "9/9 [==============================] - 0s 663us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 175/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 176/300\n",
      "9/9 [==============================] - 0s 782us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 177/300\n",
      "9/9 [==============================] - 0s 774us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 178/300\n",
      "9/9 [==============================] - 0s 665us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 179/300\n",
      "9/9 [==============================] - 0s 559us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 180/300\n",
      "9/9 [==============================] - 0s 672us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 181/300\n",
      "9/9 [==============================] - 0s 773us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 182/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 183/300\n",
      "9/9 [==============================] - 0s 555us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 184/300\n",
      "9/9 [==============================] - 0s 777us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 185/300\n",
      "9/9 [==============================] - 0s 664us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 186/300\n",
      "9/9 [==============================] - 0s 663us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 187/300\n",
      "9/9 [==============================] - 0s 776us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 188/300\n",
      "9/9 [==============================] - 0s 777us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 189/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 190/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 191/300\n",
      "9/9 [==============================] - 0s 672us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 192/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 193/300\n",
      "9/9 [==============================] - 0s 662us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 194/300\n",
      "9/9 [==============================] - 0s 559us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 195/300\n",
      "9/9 [==============================] - 0s 781us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 196/300\n",
      "9/9 [==============================] - 0s 664us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 197/300\n",
      "9/9 [==============================] - 0s 555us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 198/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 199/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 200/300\n",
      "9/9 [==============================] - 0s 556us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 201/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 202/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 203/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 204/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 205/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 206/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 207/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 208/300\n",
      "9/9 [==============================] - 0s 662us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 209/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 210/300\n",
      "9/9 [==============================] - 0s 669us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 211/300\n",
      "9/9 [==============================] - 0s 671us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 212/300\n",
      "9/9 [==============================] - 0s 556us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 213/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 214/300\n",
      "9/9 [==============================] - 0s 665us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 215/300\n",
      "9/9 [==============================] - 0s 665us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 216/300\n",
      "9/9 [==============================] - 0s 556us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 217/300\n",
      "9/9 [==============================] - 0s 665us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 218/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 219/300\n",
      "9/9 [==============================] - 0s 885us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 220/300\n",
      "9/9 [==============================] - 0s 779us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 221/300\n",
      "9/9 [==============================] - 0s 670us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 222/300\n",
      "9/9 [==============================] - 0s 669us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 223/300\n",
      "9/9 [==============================] - 0s 780us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 224/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 225/300\n",
      "9/9 [==============================] - 0s 669us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 226/300\n",
      "9/9 [==============================] - 0s 782us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 227/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 228/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 229/300\n",
      "9/9 [==============================] - 0s 891us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 230/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 231/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 232/300\n",
      "9/9 [==============================] - 0s 893us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 233/300\n",
      "9/9 [==============================] - 0s 781us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 234/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 235/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 236/300\n",
      "9/9 [==============================] - 0s 775us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 237/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 238/300\n",
      "9/9 [==============================] - 0s 890us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 239/300\n",
      "9/9 [==============================] - 0s 891us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 240/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 241/300\n",
      "9/9 [==============================] - 0s 782us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 242/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 243/300\n",
      "9/9 [==============================] - 0s 668us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 244/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 245/300\n",
      "9/9 [==============================] - 0s 671us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 246/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 247/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8747 - mse: 1.8747\n",
      "Epoch 248/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 249/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 250/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 251/300\n",
      "9/9 [==============================] - 0s 665us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 252/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 253/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 254/300\n",
      "9/9 [==============================] - 0s 774us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 255/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 256/300\n",
      "9/9 [==============================] - 0s 779us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 257/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 665us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 258/300\n",
      "9/9 [==============================] - 0s 665us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 259/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 260/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 261/300\n",
      "9/9 [==============================] - 0s 670us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 262/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 263/300\n",
      "9/9 [==============================] - 0s 555us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 264/300\n",
      "9/9 [==============================] - 0s 886us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 265/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 266/300\n",
      "9/9 [==============================] - 0s 668us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 267/300\n",
      "9/9 [==============================] - 0s 556us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 268/300\n",
      "9/9 [==============================] - 0s 783us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 269/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 270/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 271/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 272/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 273/300\n",
      "9/9 [==============================] - 0s 666us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 274/300\n",
      "9/9 [==============================] - 0s 665us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 275/300\n",
      "9/9 [==============================] - 0s 775us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 276/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 277/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 278/300\n",
      "9/9 [==============================] - 0s 775us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 279/300\n",
      "9/9 [==============================] - 0s 669us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 280/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 281/300\n",
      "9/9 [==============================] - 0s 780us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 282/300\n",
      "9/9 [==============================] - 0s 670us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 283/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 284/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 285/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 286/300\n",
      "9/9 [==============================] - 0s 558us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 287/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 288/300\n",
      "9/9 [==============================] - 0s 670us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 289/300\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 290/300\n",
      "9/9 [==============================] - 0s 774us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 291/300\n",
      "9/9 [==============================] - 0s 673us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 292/300\n",
      "9/9 [==============================] - 0s 662us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 293/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 294/300\n",
      "9/9 [==============================] - 0s 892us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 295/300\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 296/300\n",
      "9/9 [==============================] - 0s 560us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 297/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 298/300\n",
      "9/9 [==============================] - 0s 894us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 299/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8746 - mse: 1.8746\n",
      "Epoch 300/300\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.8746 - mse: 1.8746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x189090f0b00>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=[1,2,3,4,5,6,7,8,9] # 공부 시간\n",
    "y=[11,22,33,44,55,66,77,87,95] # 각 공부시간에 맵핑된 성적\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 입력 x의 차원 1 출력 y의 차원 1. 선형 회귀이므로 activation은 'linear'\n",
    "model.add(Dense(1, input_dim = 1, activation = 'linear'))\n",
    "\n",
    "# sgd는 경사하강법. learning rate 0.01\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "\n",
    "# 손실함수는 mse (평균 제곱 오차)\n",
    "model.compile(optimizer = sgd, loss = 'mse', metrics = ['mse'])\n",
    "\n",
    "# 주어진 X와 y 데이터에 대해 오차 최소화 작업 300번\n",
    "model.fit(X,y, batch_size = 1, epochs = 300, shuffle=False) # batch size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "swedish-eight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1890b0ccba8>,\n",
       " <matplotlib.lines.Line2D at 0x1890b0cccc0>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X, model.predict(X), 'b', X,y,'k.') # b: blue, k.: black point marker 이런 형식으로 이어서 써도 올라가는가봥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "altered-grave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[98.55561]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([9.5])) # 9.5시간 공부하면 몇 점 나올까"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-lucas",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
