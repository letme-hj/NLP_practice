{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "worth-spouse",
   "metadata": {},
   "source": [
    "all exercises based on [Introduction to Deep Learning for NLP](https://wikidocs.net/24949)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-growing",
   "metadata": {},
   "source": [
    "### LSA (Latent Semantic Analysis - 잠재 의미 분석)\n",
    "\n",
    "**SVD**(특이값분해) 이해, svd 기반 분석<br>\n",
    "기존 행렬 DTM -> A<br>\n",
    "- A = U X S X VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "periodic-strand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle = True, random_state=1, remove=('headers,', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "american-authority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: timmbake@mcl.ucsb.edu (Bake Timmons)\\nSubject: Re: Amusing atheists and agnostics\\nLines: 66\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "peaceful-regular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "concrete-cigarette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        From: ab4z@Virginia.EDU (\"Andi Beyer\")\\nSubjec...\n",
       "1        From: timmbake@mcl.ucsb.edu (Bake Timmons)\\nSu...\n",
       "2        From: bc744@cleveland.Freenet.Edu (Mark Ira Ka...\n",
       "3        From: ray@ole.cdac.com (Ray Berry)\\nSubject: C...\n",
       "4        From: kkeller@mail.sas.upenn.edu (Keith Keller...\n",
       "                               ...                        \n",
       "11309    From: adams@bellini.berkeley.edu (Adam L. Schw...\n",
       "11310    From: levin@bbn.com (Joel B Levin)\\nSubject: R...\n",
       "11311    From: tedward@cs.cornell.edu (Edward [Ted] Fis...\n",
       "11312    From: mori@volga.mfd.cs.fujitsu.co.jp (Tsuyosh...\n",
       "11313    From: marc@yogi.austin.ibm.com (Marc J. Stephe...\n",
       "Name: data, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(documents, columns = ['data'])\n",
    "df['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-slovenia",
   "metadata": {},
   "source": [
    "### 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "individual-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수문자 제거\n",
    "df['data'] = df['data'].str.replace('[^A-Za-z]',' ')\n",
    "\n",
    "# 길이 짧은 단어 제거\n",
    "df['data'] = df['data'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# 전체 단어 소문자화\n",
    "df['data'] = df['data'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "divine-concept",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        from virginia andi beyer subject israeli terro...\n",
       "1        from timmbake ucsb bake timmons subject amusin...\n",
       "2        from cleveland freenet mark kaufman subject re...\n",
       "3        from cdac berry subject clipper business usual...\n",
       "4        from kkeller mail upenn keith keller subject p...\n",
       "                               ...                        \n",
       "11309    from adams bellini berkeley adam schwartz subj...\n",
       "11310    from levin joel levin subject selective placeb...\n",
       "11311    from tedward cornell edward fischer subject be...\n",
       "11312    from mori volga fujitsu tsuyoshi mori subject ...\n",
       "11313    from marc yogi austin marc stephenson subject ...\n",
       "Name: data, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "psychological-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "df['data'] = df['data'].apply(lambda x: x.split()) # 토큰화..\n",
    "df['data'] = df['data'].apply(lambda x: ' '.join([w for w in x if w not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "senior-edition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        virginia andi beyer subject israeli terrorism ...\n",
       "1        timmbake ucsb bake timmons subject amusing ath...\n",
       "2        cleveland freenet mark kaufman subject rejoind...\n",
       "3        cdac berry subject clipper business usual arti...\n",
       "4        kkeller mail upenn keith keller subject playof...\n",
       "                               ...                        \n",
       "11309    adams bellini berkeley adam schwartz subject d...\n",
       "11310    levin joel levin subject selective placebo lin...\n",
       "11311    tedward cornell edward fischer subject best ho...\n",
       "11312    mori volga fujitsu tsuyoshi mori subject want ...\n",
       "11313    marc yogi austin marc stephenson subject astro...\n",
       "Name: data, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-collect",
   "metadata": {},
   "source": [
    "### TF-IDF 행렬 만들기\n",
    "TfidfVectorizer은 기본적으로 토큰화되지 않은 텍스트 데이터를 입력으로 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "transsexual-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 1000, max_df = 0.5, smooth_idf = True)\n",
    "# max_df : 0.5(반) 이상의 문서에 등장한 단어는 무시할 것! 제외할 것! (0.0-1.0 까지의 수를 넣으면 전체 문서 대비 문서 비율이 됨)\n",
    "X = vectorizer.fit_transform(df['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "healthy-romance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-extraction",
   "metadata": {},
   "source": [
    "### 토픽모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "juvenile-price",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 행렬 분해 (Truncated SVD 사용할 것)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_model = TruncatedSVD(n_components=20, algorithm = 'randomized', n_iter = 100, random_state = 122)\n",
    "# n_iter은 뭐지?\n",
    "# n_iter : int, default=5\n",
    "#    Number of iterations for randomized SVD solver. Not used by ARPACK. The\n",
    "#    default is larger than the default in\n",
    "#    :func:`~sklearn.utils.extmath.randomized_svd` to handle sparse\n",
    "#    matrices that may have large slowly decaying spectrum.\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "familiar-spoke",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(svd_model.components_) # components가 SVD 분해 시 Vt에 해당함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "imposed-mongolia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names() # 단어집합 (1000개)\n",
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "moved-assumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:  [('posting', 0.2305), ('host', 0.22474), ('nntp', 0.22458), ('university', 0.19165), ('would', 0.18322)]\n",
      "Topic 2:  [('nntp', 0.41933), ('host', 0.41674), ('posting', 0.41392), ('university', 0.19544), ('distribution', 0.15948)]\n",
      "Topic 3:  [('windows', 0.36606), ('card', 0.17801), ('thanks', 0.16612), ('file', 0.15601), ('drive', 0.14655)]\n",
      "Topic 4:  [('university', 0.46631), ('state', 0.31673), ('ohio', 0.21558), ('virginia', 0.15955), ('pitt', 0.15677)]\n",
      "Topic 5:  [('pitt', 0.46765), ('gordon', 0.40777), ('banks', 0.3984), ('computer', 0.22913), ('science', 0.21493)]\n",
      "Topic 6:  [('cleveland', 0.31501), ('cwru', 0.30519), ('freenet', 0.21488), ('western', 0.17831), ('reserve', 0.17686)]\n",
      "Topic 7:  [('nasa', 0.30388), ('access', 0.28613), ('cleveland', 0.26006), ('ohio', 0.23071), ('state', 0.22779)]\n",
      "Topic 8:  [('nasa', 0.54022), ('space', 0.31578), ('cleveland', 0.20301), ('cwru', 0.16288), ('freenet', 0.12835)]\n",
      "Topic 9:  [('access', 0.27712), ('sale', 0.26642), ('drive', 0.26123), ('game', 0.16652), ('digex', 0.16502)]\n",
      "Topic 10:  [('access', 0.41486), ('windows', 0.24982), ('digex', 0.24684), ('virginia', 0.20354), ('unix', 0.14154)]\n",
      "Topic 11:  [('distribution', 0.37687), ('world', 0.32139), ('reply', 0.21858), ('virginia', 0.19487), ('sale', 0.14502)]\n",
      "Topic 12:  [('clipper', 0.32549), ('chip', 0.29691), ('encryption', 0.2217), ('netcom', 0.16637), ('reply', 0.14692)]\n",
      "Topic 13:  [('virginia', 0.59314), ('university', 0.1728), ('caltech', 0.1397), ('technology', 0.1317), ('space', 0.12111)]\n",
      "Topic 14:  [('sale', 0.33965), ('caltech', 0.30418), ('institute', 0.26238), ('keith', 0.23199), ('technology', 0.22611)]\n",
      "Topic 15:  [('windows', 0.33874), ('caltech', 0.29412), ('virginia', 0.25221), ('institute', 0.22227), ('technology', 0.21874)]\n",
      "Topic 16:  [('virginia', 0.38425), ('sale', 0.33795), ('netcom', 0.24381), ('windows', 0.11583), ('card', 0.11156)]\n",
      "Topic 17:  [('netcom', 0.50694), ('uiuc', 0.26773), ('university', 0.24645), ('illinois', 0.24034), ('services', 0.21432)]\n",
      "Topic 18:  [('netcom', 0.4818), ('drive', 0.25194), ('virginia', 0.23172), ('services', 0.19895), ('state', 0.14894)]\n",
      "Topic 19:  [('card', 0.36817), ('would', 0.25002), ('video', 0.19768), ('distribution', 0.18263), ('world', 0.1531)]\n",
      "Topic 20:  [('computer', 0.44868), ('science', 0.2918), ('purdue', 0.15797), ('department', 0.1575), ('netcom', 0.13848)]\n"
     ]
    }
   ],
   "source": [
    "def get_topics(components, feature_names, n =5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic {}: \".format(idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]]) # -1부터 -n-1까지 차례로 출력\n",
    "get_topics(svd_model.components_, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "contained-pocket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 4]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-guidance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
