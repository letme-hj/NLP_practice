{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "assumed-horse",
   "metadata": {},
   "source": [
    "all exercises based on [Introduction to Deep Learning for NLP](https://wikidocs.net/31766)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-reasoning",
   "metadata": {},
   "source": [
    "텍스트를 숫자(정수)로 표현하는 것은 원핫인코딩/워드임베딩 등과 관련 있음. 실제로 어떤 과정으로 단어 텍스트에 정수 인덱스를 부여할 수 있는지 정리할 것.<br>\n",
    "\n",
    "방법 중 하나: 단어를 빈도수 순으로 정렬한 단어 집합에 많은 단어부터 차례로 정수 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-encoding",
   "metadata": {},
   "source": [
    "### dict 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thermal-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "australian-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "positive-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 토큰화\n",
    "sent = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "enabling-stable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
      "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "# 정제 작업과 함께\n",
    "# 단어 토큰화\n",
    "clean_words = []\n",
    "stop_words = stopwords.words('english')\n",
    "# 단어별 빈도수 기록\n",
    "vocab = {}\n",
    "\n",
    "for i in sent:\n",
    "    words = i.lower()\n",
    "    words = word_tokenize(words)\n",
    "    clean = []\n",
    "    for w in words:\n",
    "        if (w not in stop_words) & (len(w)>2): # 글자수 2 이하는 제외\n",
    "            clean.append(w)\n",
    "            if w not in vocab:\n",
    "                vocab[w] = 1\n",
    "            else: vocab[w] += 1\n",
    "    clean_words.append(clean)\n",
    "print(clean_words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "certified-bookmark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8),\n",
       " ('secret', 6),\n",
       " ('huge', 5),\n",
       " ('kept', 4),\n",
       " ('person', 3),\n",
       " ('word', 2),\n",
       " ('keeping', 2),\n",
       " ('good', 1),\n",
       " ('knew', 1),\n",
       " ('driving', 1),\n",
       " ('crazy', 1),\n",
       " ('went', 1),\n",
       " ('mountain', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도 높은 순 정렬\n",
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True) # dict.items는 (key, value)의 튜플로 return\n",
    "vocab_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bacterial-reward",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1,\n",
       " 'secret': 2,\n",
       " 'huge': 3,\n",
       " 'kept': 4,\n",
       " 'person': 5,\n",
       " 'word': 6,\n",
       " 'keeping': 7}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도순 인덱스 부여\n",
    "word_to_index = {}\n",
    "for i in range(len(vocab_sorted)):\n",
    "    if vocab_sorted[i][1]>=2: # 빈도수 2 미만인 건 제외\n",
    "        word_to_index[vocab_sorted[i][0]] = i+1\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "determined-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존의 token들을 전부 인코딩할 건데, 빈도수 때문에 삭제된 단어들이 있음.\n",
    "# 그 단어들을 하나의 단어 집합으로 생각하고 인덱스 부여 (etc처럼)\n",
    "word_to_index['OOV'] = len(word_to_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "royal-copying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [8, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 8, 1, 8], [1, 8, 3, 8]]\n"
     ]
    }
   ],
   "source": [
    "encoded = []\n",
    "for i in clean_words:\n",
    "    # encoded_sent = [ind for j in i for e,ind in word_to_index.items() if e==j]\n",
    "    encoded_sent = []\n",
    "    for j in i:\n",
    "        if j in word_to_index.keys():\n",
    "            encoded_sent.append(word_to_index[j])\n",
    "        else:\n",
    "            encoded_sent.append(word_to_index['OOV'])\n",
    "    encoded.append(encoded_sent)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-singapore",
   "metadata": {},
   "source": [
    "### 더 쉽게 Counter 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "based-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "greenhouse-provincial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "available-feelings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "# 위의 단어 집합을 하나로 만들어 줄 것 (내부 리스트 경계 없앨 것)\n",
    "words = sum(clean_words, []) \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cutting-bobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter(words)\n",
    "print(vocab)\n",
    "# Counter 객체에 리스트 내 원소의 빈도수가 알아서 저장됨 (Counter - dict 형태)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "earlier-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 등장 빈도 수가 높은 상위 5개 지정\n",
    "vocab_most = vocab.most_common(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "complimentary-defensive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "disabled-tactics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정수 인덱스 부여\n",
    "word_to_index = {}\n",
    "ind = 1\n",
    "for i, j in vocab_most:\n",
    "    word_to_index[i] = ind\n",
    "    ind +=1\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-budapest",
   "metadata": {},
   "source": [
    "### nltk의 FreqDist 사용\n",
    "Counter와 비슷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "patent-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cross-controversy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 13 samples and 36 outcomes>\n"
     ]
    }
   ],
   "source": [
    "# np.hstack으로 문장 구분 제거 후 입력으로 사용 (위에서는 sum(리스트, [])로 했었음)\n",
    "\n",
    "vocab = FreqDist(np.hstack(clean_words))\n",
    "print(vocab) # 프린트는 안됨 FreqDist 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "patient-leader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab['barber']) # 인덱싱은 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "original-affect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_most = vocab.most_common(5) # Counter 객체랑 똑같이 most_common 사용 가능, return 값은 리스트\n",
    "vocab_most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "legitimate-watson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# 인덱스 부여.\n",
    "word_to_index = {word[0]: ind+1 for ind, word in enumerate(vocab_most)} # dict comprehension (방식은 list comprehension과 같음)\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "organizational-spokesman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enumerate object at 0x0000022CF7E1FDC8>\n"
     ]
    }
   ],
   "source": [
    "print(enumerate(vocab_most))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-theorem",
   "metadata": {},
   "source": [
    "### cf. enumerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-presence",
   "metadata": {},
   "source": [
    "순서가 있는 자료형을 입력으로 받아, 인덱스를 순차적으로 함께 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "impressive-tolerance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, (1, 'one')) one\n",
      "(1, (2, 'two')) two\n",
      "(2, (3, 'three')) three\n",
      "(3, (4, 'four')) four\n"
     ]
    }
   ],
   "source": [
    "sample = [(1,'one'), (2, 'two'), (3, 'three'), (4, 'four')]\n",
    "for i in enumerate(sample):\n",
    "    print(i, sample[i[0]][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-value",
   "metadata": {},
   "source": [
    "### Keras 전처리 도구 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "animal-exploration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "compressed-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_words) # fit_on_texts 인자로 코퍼스를 입력하면, 빈도수 기준으로 단어 집합 생성\n",
    "# 이중리스트도 상관 없나봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fundamental-movement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1,\n",
       " 'secret': 2,\n",
       " 'huge': 3,\n",
       " 'kept': 4,\n",
       " 'person': 5,\n",
       " 'word': 6,\n",
       " 'keeping': 7,\n",
       " 'good': 8,\n",
       " 'knew': 9,\n",
       " 'driving': 10,\n",
       " 'crazy': 11,\n",
       " 'went': 12,\n",
       " 'mountain': 13}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index # 알아서 빈도순으로 인덱스 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "scientific-decline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('barber', 8),\n",
       "             ('person', 3),\n",
       "             ('good', 1),\n",
       "             ('huge', 5),\n",
       "             ('knew', 1),\n",
       "             ('secret', 6),\n",
       "             ('kept', 4),\n",
       "             ('word', 2),\n",
       "             ('keeping', 2),\n",
       "             ('driving', 1),\n",
       "             ('crazy', 1),\n",
       "             ('went', 1),\n",
       "             ('mountain', 1)])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts # 빈도 확인도 가능. 알아서 저장됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "comparable-diversity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 8, 5],\n",
       " [1, 3, 5],\n",
       " [9, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [7, 7, 3, 2, 10, 1, 11],\n",
       " [1, 12, 3, 13]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(clean_words) # 이렇게 이미 정해진 index(tokenizer에 들어있는 정보)를 반영해 기존 문장(입력)을 정수 리스트로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "rational-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 5개 단어만\n",
    "tokenizer = Tokenizer(num_words = 5+1) # 상위 5개 단어만 사용. num_words는 0개부터 카운트하기 때문에\n",
    "tokenizer.fit_on_texts(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fitting-offer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1,\n",
       " 'secret': 2,\n",
       " 'huge': 3,\n",
       " 'kept': 4,\n",
       " 'person': 5,\n",
       " 'word': 6,\n",
       " 'keeping': 7,\n",
       " 'good': 8,\n",
       " 'knew': 9,\n",
       " 'driving': 10,\n",
       " 'crazy': 11,\n",
       " 'went': 12,\n",
       " 'mountain': 13}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index # num_words 설정했는데도 인덱스는 다 저장됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "backed-permission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 5],\n",
       " [1, 3, 5],\n",
       " [2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4],\n",
       " [1, 4],\n",
       " [1, 4, 2],\n",
       " [3, 2, 1],\n",
       " [1, 3]]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(clean_words) # num_words 설정한 게 반영되어 인덱스 6 이상의 단어들(비교적 빈도가 적은)이 삭제됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "discrete-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도 적은 단어들 OOV로 보존하고 싶다면!\n",
    "tokenizer = Tokenizer(num_words = 5+2, oov_token = 'OOV')\n",
    "tokenizer.fit_on_texts(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "existing-jungle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV의 인덱스:  1\n"
     ]
    }
   ],
   "source": [
    "print('OOV의 인덱스: ', tokenizer.word_index['OOV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "noble-slovak",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(clean_words)) # 빈도 상위 5개는 2~6, OOV는 1로 인덱싱 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-influence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
