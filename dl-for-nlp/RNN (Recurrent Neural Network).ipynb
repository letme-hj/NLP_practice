{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "creative-dietary",
   "metadata": {},
   "source": [
    "all exercises based on [Introduction to Deep Learning for NLP](https://wikidocs.net/22886)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-australia",
   "metadata": {},
   "source": [
    "- 기본 형태\n",
    "`model.add(SimpleRNN(hidden_size))`\n",
    "- 추가 인자 사용\n",
    "`model.add(SimpleRNN(hidden_size, input_length=M, input_dim=N))`\n",
    "\n",
    "    - hidden_size = 은닉 상태의 크기. 메모리 셀이 다음 시점의 메모리 셀과 출력층으로 보내는 값의 크기(output_dim)와도 동일. RNN의 용량(capacity)을 늘린다고 보면 되고, 중소형 모델은 보통 128, 256, 512, 1024 등의 값 가짐\n",
    "    - timesteps = 입력 시퀀스의 길이(input_length). 시점의 수\n",
    "    - input_dim = 입력의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fifteen-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "medical-officer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, input_shape = (2,10)))\n",
    "# model.add(SimpleRNN(3, input_length = 2, input_dim = 10))\n",
    "model.summary()\n",
    "# input length vs input dim?\n",
    "# batch size를 지금 알 수 없음 -> None\n",
    "# hidden size: 3 (=output dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "under-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (8, 3)                    42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# batch size를 미리 정의해보자\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, batch_input_shape = (8,2,10)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continent-immune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (8, 2, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# time step 포함해 3d 텐서 리턴하도록 모델을 만들어보자\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, batch_input_shape = (8,2,10), return_sequences = True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-underwear",
   "metadata": {},
   "source": [
    "### 파이썬으로 RNN 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "agricultural-sharp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "timesteps = 10 # 시점의 수 (문장의 길이)\n",
    "input_dim = 4 # 입력의 차원 (단어 벡터의 차원)\n",
    "hidden_size = 8 # 은닉 상태의 크기. 메모리 셀 용량\n",
    "\n",
    "inputs = np.random.random((timesteps, input_dim)) # 입력에 해당되는 2d 텐서\n",
    "\n",
    "hidden_state_t = np.zeros((hidden_size,)) # 초기 은닉 상태는 영벡터로 초기화\n",
    "# 은닉 상태의 크기 hidden_size로 은닉 상태 만듦\n",
    "\n",
    "print(hidden_state_t) # 8의 크기를 가지는 은닉 상태."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "shared-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx = np.random.random((hidden_size, input_dim)) # (8,4) 크기의 2d 텐서. 입력에 대한 가중치\n",
    "Wh = np.random.random((hidden_size, hidden_size)) # (8,8) 크기의 2d 텐서 생성. 은닉 상태에 대한 가중치\n",
    "b = np.random.random((hidden_size,)) # (8, )크기의 1d 텐서 생성 (이 값은 편향)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "modular-wings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "(8, 8)\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(Wx))\n",
    "print(np.shape(Wh))\n",
    "print(np.shape(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ordered-excerpt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "(2, 8)\n",
      "(3, 8)\n",
      "(4, 8)\n",
      "(5, 8)\n",
      "(6, 8)\n",
      "(7, 8)\n",
      "(8, 8)\n",
      "(9, 8)\n",
      "(10, 8)\n",
      "[[0.8509924  0.95215477 0.94177424 0.46921501 0.80442601 0.66239114\n",
      "  0.71711232 0.91959919]\n",
      " [0.99996761 0.99973676 0.99972805 0.99974395 0.99993515 0.9996309\n",
      "  0.99984282 0.99989549]\n",
      " [0.99999569 0.99991491 0.99996002 0.99996578 0.9999905  0.99986491\n",
      "  0.99997427 0.99998605]\n",
      " [0.99999736 0.99992305 0.9999573  0.99997255 0.99998806 0.99990229\n",
      "  0.99997999 0.99998443]\n",
      " [0.9999888  0.99986317 0.99988598 0.99996787 0.99999292 0.99984271\n",
      "  0.99996274 0.9999767 ]\n",
      " [0.99999745 0.99977763 0.99996415 0.99996419 0.99996028 0.99991444\n",
      "  0.99996425 0.99998142]\n",
      " [0.99999655 0.99969116 0.99990581 0.99996511 0.99994811 0.99980613\n",
      "  0.9999707  0.99989551]\n",
      " [0.99999322 0.99869635 0.99991179 0.99993435 0.9998285  0.99963675\n",
      "  0.99992688 0.9998003 ]\n",
      " [0.99999658 0.99950968 0.99990967 0.99996559 0.99992256 0.99988789\n",
      "  0.99995736 0.99993329]\n",
      " [0.99999784 0.99965742 0.99996927 0.99995446 0.99991575 0.99986186\n",
      "  0.99996269 0.99996127]]\n"
     ]
    }
   ],
   "source": [
    "total_hidden_states = []\n",
    "\n",
    "# 메모리 셀 동작\n",
    "for input_t in inputs:\n",
    "    output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t) + b)\n",
    "    total_hidden_states.append(list(output_t)) # 각 시점의 은닉 상태의 값을 계속 축적\n",
    "    print(np.shape(total_hidden_states))\n",
    "    hidden_state_t = output_t\n",
    "\n",
    "total_hidden_states = np.stack(total_hidden_states, axis = 0)\n",
    "\n",
    "print(total_hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-transparency",
   "metadata": {},
   "source": [
    "### 깊은 순환 신경망 (Deep Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "signed-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(hidden_size, return_sequences = True))\n",
    "model.add(SimpleRNN(hidden_size, return_sequences = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-treaty",
   "metadata": {},
   "source": [
    "### 양방향 순환 신경망 (Bidirectional Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "parliamentary-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True), input_shape = (timesteps, input_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-matrix",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
